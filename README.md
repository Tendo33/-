# 面试题

## 深度学习

### 模型评估方法

- **Accuracy作为指标有哪些局限性？**
    
    准确率是样本分类问题中最简单也是最直观的评价指标。但存在明显的缺陷。比如负样本占99%时，分类器把所以样本都预测为负样本也可以获得99%的准确率。所以，当**不同类别的样本比例非常不均衡**时，占比大的类别往往成为影响准确率的最主要因素，此时准确率指标并不足以说明分类器的好坏。
    
- **ROC曲线和PR曲线各是什么？**
    - PR曲线中的P代表的是`precision（精准率）`，R代表的是`recall（召回率）`，其代表的是精准率与召回率的关系，一般情况下，将recall设置为横坐标，precision设置为纵坐标。
    - 在ROC曲线中，**横轴是假正例率（FPR），纵轴是真正例率（TPR） = recall** 。
    - AUC (Area under Curve)：ROC曲线下的面积，介于0.1和1之间，作为数值可以直观的评价分类器的好坏，值越大越好。AUC = 1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。
- 编程实现AUC的计算，并指出复杂度？
    
    ```python
    import numpy as np
    def auc_calculate(labels, preds, n_bins=100):
        postive_len = sum(labels)  # 正样本数量（因为正样本都是1）
        negative_len = len(labels) - postive_len  # 负样本数量
        total_case = postive_len * negative_len  # 正负样本对
        pos_histogram = [0 for _ in range(n_bins)]
        neg_histogram = [0 for _ in range(n_bins)]
        bin_width = 1.0 / n_bins
        for i in range(len(labels)):
            nth_bin = int(preds[i] / bin_width)
            if labels[i] == 1:
                pos_histogram[nth_bin] += 1
            else:
                neg_histogram[nth_bin] += 1
        accumulated_neg = 0
        satisfied_pair = 0
        for i in range(n_bins):
            satisfied_pair += (pos_histogram[i] * accumulated_neg + pos_histogram[i] * neg_histogram[i] * 0.5)
            accumulated_neg += neg_histogram[i]
            # print(i,satisfied_pair,accumulated_neg)
        return satisfied_pair / float(total_case)
    ```
    
- **AUC指标有什么特点？放缩结果对AUC是否有影响？**
    - AUC（Area Under the Curve）指标是用于衡量二分类模型性能的一种指标，其特点如下：
        1. AUC指标不受正负样本比例影响：AUC指标的计算过程中，首先对样本进行排序，然后根据样本的预测概率值，计算ROC曲线，最后计算ROC曲线下的面积，因此不会受到正负样本比例的影响。
        2. AUC指标对样本分类阈值不敏感：AUC指标的计算过程中，不需要设置分类阈值，因此不会受到分类阈值的选择的影响，具有较强的鲁棒性。
        3. AUC指标能够很好地衡量模型的整体性能：AUC指标的取值范围为0.5到1.0，当AUC指标的取值越接近1.0时，说明模型的性能越好。
        
        对于放缩结果是否对AUC有影响，一般情况下不会影响AUC的计算结果。因为AUC是基于样本的预测概率值进行计算的，而对样本的预测概率值进行放缩并不会改变样本之间的排序关系。但是，在一些特殊情况下，放缩结果可能会对AUC的计算结果产生影响，如使用了一些不合理的放缩方法，导致样本的预测概率值发生了错误的变化。因此，在进行放缩操作时，需要谨慎考虑，确保不会对AUC的计算结果产生影响。
        
- **余弦距离与欧式距离有什么特点？**
    - 欧氏距离衡量的是空间各点的绝对距离，跟各个点所在的位置坐标直接相关；而余弦距离衡量的是空间向量的夹角，更加体现在方向上的差异，而不是位置。
    - 欧氏距离能够体现个体数值特征的绝对差异，所以更多的用于需要从维度的数值大小中体现差异的分析，如使用用户行为指标分析用户价值的相似度或差异。
    - 余弦距离更多的是从方向上区分差异，而对绝对的数值不敏感，更多的用于使用用户对内容评分来区分兴趣的相似度和差异，同时修正了用户间可能存在的度量标准不统一的问题（因为余弦距离对绝对数值不敏感）
    - 余弦距离可以知道，余弦距离的取值范围为[0,2] ,这就满足了非负性的性质，欧式距离取值范围>=0

### 基本方法

- **如何划分训练集？如何选取验证集？**
    
    一般来说，我们将数据集划分为训练集、验证集和测试集三部分。其中，训练集用于训练模型，验证集用于评估模型的性能和调整模型参数，测试集用于最终评估模型的性能。
    
    - 留出法（Hold-out）：将数据集划分为训练集和验证集两部分。一般来说，将数据集的70% ~ 80%作为训练集，剩余的20% ~ 30%作为验证集。
    - 交叉验证法（Cross-validation）：将数据集划分为k个子集，其中k-1个子集用于训练模型，剩余的一个子集用于验证模型。将k个子集分别作为验证集，重复k次，最终得到k个模型性能的评估结果。
    - 自助法（Bootstrapping）：从数据集中随机抽取一个样本，并将其放入训练集中，然后将样本放回数据集中，继续进行下一次抽样。这样，训练集中可能包含重复的样本，验证集中则包含未被选中的样本。
        1. 验证集应该与测试集和实际数据具有相似的分布。
        2. 验证集的大小应该足够大，以便对模型进行准确的评估。
        3. 验证集应该与训练集互斥，即在训练集中没有出现过的样本应该被包含在验证集中。
        4. 如果使用交叉验证法，验证集应该尽可能包含数据集中的所有样本，以便对模型进行全面的评估。
- **什么是偏差和方差？**
    1. 误差（ Error） = 偏差（ Bias） + 方差（ Variance） + 噪声（ Noise），一般地，我们把机器学习模型的预测输出与样本的真实label之间的差异称为误差，其反应的是整个模型的准确度。
    2. 噪声（ Noise）：描述了在当前任务上任何机器学习算法所能达到的期望泛化误差的下界，即刻画了当前任务本质的难度。
    3. 偏差（ Bias）：衡量了模型拟合训练数据的能力，偏差反应的是所有采样得到的大小相同的训练集训练出的所有模型的输出平均值和真实label之间的偏差，即模型本身的精确度。偏差通常是由于我们对机器学习算法做了错误的假设所导致的，比如真实数据分布映射的是某个 二次函数，但我们假设模型是一次函数。
    4. 偏差（ Bias）越小，拟合能力却强（可能产生过拟合）；反之，拟合能力越弱（可能产生欠拟合） 。偏差越大，越偏离真实数据。
    5. 方差描述的是预测值的变化范围，离散程度，也就是离期望值的距离。 方差越大，数据的分布越分散，模型的稳定程度越差。
    6. 方差也反应了模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。 由方差带来的 误差通常体现在测试误差相对于训练误差的增量上。
    7. 方差通常是由于模型的复杂度相对于训练样本数过高导致的。 方差越小，模型的泛化能力越高； 反之，模型的泛化能力越低。如果模型在训练集上拟合效果比较优秀，但是在测试集上拟合效果比较差， 则表示方差较大， 说明模型的稳定程度较差，出现这种现象可能是由于模型对训练集过拟合造成的。
- **什么是过拟合？深度学习解决过拟合的方法有哪些？**
    
    过拟合（Overfitting）是指机器学习算法在训练数据上表现良好，但在测试数据上表现差的现象。通常发生在模型过于复杂、训练数据量较少、训练次数过多等情况下。
    
    深度学习解决过拟合的方法有以下几种：
    
    1. 增加训练数据：通过增加训练数据，可以有效地降低模型过拟合的风险。这是最有效的方法之一，但有时候增加训练数据并不是一件容易的事情。
    2. 正则化（Regularization）：通过在模型的损失函数中添加正则项来限制模型的复杂度，例如L1正则化、L2正则化等。正则化可以强制模型更加平滑，减少模型对噪声的敏感性。
    3. Dropout：Dropout是一种在训练过程中随机删除一部分神经元的方法。这样可以使得模型不能过于依赖某些特征，从而降低过拟合的风险。
    4. 提前停止（Early stopping）：在训练过程中，可以在验证集的性能不再提高时停止训练，从而避免过拟合。这个方法有时可以在训练速度和模型效果之间做出一个平衡。
    5. 数据增强（Data augmentation）：通过对原始训练数据进行旋转、平移、缩放等变换，生成更多的训练数据。这样可以增加数据的多样性，帮助模型更好地学习。
    6. 模型集成（Model ensemble）：通过将多个模型的预测结果进行融合，可以得到更准确的结果。这个方法需要训练多个模型，因此比较耗时和复杂。
- **深度模型参数调整的一般方法论？**
    
    深度模型参数调整的一般方法论包括以下几个步骤：
    
    1. 确定模型结构：选择合适的模型结构对于模型性能的提高非常重要。根据任务的不同，可以选择不同的模型结构，如卷积神经网络、循环神经网络、残差网络等。
    2. 准备数据集：收集、准备和预处理数据集，包括数据清洗、数据预处理、数据增强等操作。
    3. 划分数据集：将数据集划分为训练集、验证集和测试集，用于模型的训练、调整和测试。
    4. 选择损失函数：根据任务的不同选择合适的损失函数，如均方误差、交叉熵等。
    5. 选择优化算法：选择合适的优化算法，如随机梯度下降、Adam、Adagrad等。
    6. 训练模型：使用训练集对模型进行训练，调整模型的参数。
    7. 调整模型超参数：调整模型的超参数，如学习率、正则化参数、批大小等，以提高模型的性能。
    8. 验证模型：使用验证集对模型进行验证，调整模型的参数和超参数，以避免模型的过拟合和欠拟合。
    9. 测试模型：使用测试集对模型进行测试，评估模型的性能。
    10. 模型部署：将训练好的模型部署到实际应用中，实现预测和分类等功能。
    
    总之，深度模型参数调整需要进行多次实验和调整，通过不断调整模型的参数和超参数，找到最优的模型，从而提高模型的性能和鲁棒性。
    

### 优化方法

- **简述了解的优化器，发展综述？**
    
    优化器是深度学习中用于优化模型参数的算法。随着深度学习的发展，越来越多的优化器被提出并应用于不同类型的神经网络，以提高训练效率和准确性。下面是一些常见的优化器和它们的发展历程：
    
    1. 梯度下降法（Gradient Descent, GD）：是优化器中最基础的一种，它通过计算损失函数的梯度来更新模型参数，使得损失函数下降。随着深度学习的发展，衍生出了批量梯度下降（Batch Gradient Descent, BGD）、随机梯度下降（Stochastic Gradient Descent, SGD）和小批量梯度下降（Mini-batch Gradient Descent, MBGD）等变种。
    2. 动量法（Momentum）：在梯度下降的基础上引入了动量，以加速学习的过程。在更新参数时，除了考虑当前梯度，还考虑之前的梯度方向，并对之前的梯度方向进行加权平均。这种方法可以减小梯度方向的变化，从而使得训练更加平滑。
    3. 自适应学习率算法：自适应学习率算法可以根据梯度的大小来自动调整学习率，从而更好地适应不同的数据集和网络结构。其中，Adagrad、Adadelta、RMSprop 和 Adam 等是比较流行的算法。
- **常用的损失函数有哪些？分别适用于什么场景？**
    
    常用的损失函数有很多种，不同的损失函数适用于不同的机器学习任务和模型。下面介绍一些常见的损失函数及其适用场景。
    
    1. 均方误差（Mean Squared Error，MSE）：MSE是回归任务中最常见的损失函数之一，计算预测值与真实值之间的平均差的平方。适用于输出连续数值的回归任务。
    2. 平均绝对误差（Mean Absolute Error，MAE）：MAE是回归任务中另一种常见的损失函数，计算预测值与真实值之间的平均绝对差。与MSE相比，MAE更适合对异常值不敏感的情况。
    3. 交叉熵损失函数（Cross-entropy Loss）：交叉熵损失函数是分类任务中最常用的损失函数之一，通过计算真实标签与预测标签之间的交叉熵来度量分类器的性能。适用于多分类和二分类任务。
    4. 二元交叉熵损失函数（Binary Cross-entropy Loss）：二元交叉熵损失函数是二分类任务中常用的损失函数，计算真实标签与预测标签之间的交叉熵。与交叉熵损失函数类似，但适用于只有两个类别的分类任务。
    5. 对数损失函数（Logarithmic Loss）：对数损失函数是用于二分类任务的另一种损失函数，计算预测标签为真实标签的概率的对数的相反数。适用于二分类任务，尤其是当模型输出概率时。
    6. Hinge损失函数：Hinge损失函数是用于支持向量机（SVM）的损失函数，通过计算误分类点到超平面的距离来度量SVM的性能。适用于二分类任务。
    7. KL散度损失函数（Kullback-Leibler Divergence Loss）：KL散度损失函数用于测量两个概率分布之间的差异。适用于生成模型的训练，如自编码器和生成对抗网络。
    8. Triplet损失函数：Triplet损失函数是用于人脸识别和图像检索任务的损失函数，通过最小化同一类别的样本之间的距离，并最大化不同类别的样本之间的距离来学习特征空间。
    
    总的来说，不同的损失函数适用于不同的机器学习任务和模型。在实际应用中，我们需要根据具体的问题来选择合适的损失函数。
    
- **梯度下降与拟牛顿法的异同？**
    
    梯度下降和拟牛顿法都是优化算法，用于求解机器学习中的损失函数的最小值。它们的主要区别在于更新参数时计算目标函数梯度的方式不同。
    
    梯度下降是一种基于一阶导数的优化算法，它通过计算目标函数在当前参数位置的梯度方向，并沿着该方向更新参数。具体来说，假设当前的参数为*θ*，损失函数为*J*(*θ*)，则梯度下降算法的迭代公式为：
    
    $*θ(t + 1) = θ(t) − η∇θJ(θ(t))*$
    
    其中，*η*是学习率，表示更新的步长大小，∇*θJ*(*θ*(*t*))表示目标函数在当前参数位置*θ*(*t*)的梯度。梯度下降的缺点在于，由于每次更新只考虑了当前位置的梯度，因此可能会出现步长过大或过小的情况，导致优化过程过程缓慢或者错过最优解。
    
    拟牛顿法是一种基于二阶导数的优化算法，它通过估计目标函数的Hessian矩阵，来确定更新参数的方向和步长。具体来说，假设当前的参数为*θ*，损失函数为*J*(*θ*)，则拟牛顿法的迭代公式为：
    
    $*θ(t + 1) = θ(t) − H − 1∇θJ(θ(t))*$
    
    其中，*H*是Hessian矩阵的近似值，通常使用BFGS算法或L-BFGS算法来估计。由于拟牛顿法考虑了目标函数的二阶导数信息，因此可以更快地收敛到最优解，并且可以自适应地调整步长大小。
    
    总的来说，梯度下降和拟牛顿法都是优化算法，用于求解机器学习中的损失函数的最小值，它们的主要区别在于参数更新时计算目标函数梯度的方式不同。梯度下降只考虑了一阶导数，计算简单，但可能出现步长过大或过小的情况，收敛速度较慢；拟牛顿法考虑了二阶导数信息，可以更快地收敛到最优解，但计算复杂度较高。
    
- **L1和L2正则分别有什么特点？为何L1稀疏？**
    
    L1和L2正则是机器学习中常用的正则化方法，它们通过对模型的参数进行约束，可以有效防止过拟合。它们的主要区别在于对参数的惩罚方式不同。
    
    L1正则（L1 regularization），也称为Lasso正则化，它通过将模型参数的L1范数作为正则化项来约束模型的复杂度。具体来说，L1正则的目标函数为：
    
    $\min\limits_{\theta}\ \frac{1}{m}\sum\limits_{i=1}^{m}{L(y_i,f(x_i,\theta))} + \lambda\sum\limits_{j=1}^{n}{|\theta_j|}$
    
    其中，*m*表示样本数量，*n*表示模型的参数数量，*L*表示损失函数，*yi*和*xi*分别表示第*i*个样本的标签和特征，*θ*表示模型参数，*λ*表示正则化强度。可以看到，L1正则对模型参数的惩罚项是每个参数的绝对值之和，这会使得一些参数变成0，因此可以用于特征选择，即可以用于降维和过滤掉不相关的特征。
    
    L2正则（L2 regularization），也称为Ridge正则化，它通过将模型参数的L2范数作为正则化项来约束模型的复杂度。具体来说，L2正则的目标函数为：
    
    $\min\limits_{\theta}\ \frac{1}{m}\sum\limits_{i=1}^{m}{L(y_i,f(x_i,\theta))} + \lambda\sum\limits_{j=1}^{n}{\theta_j^2}$
    
    可以看到，L2正则对模型参数的惩罚项是每个参数的平方和，这会使得每个参数都趋向于接近于0，但不会变成0，因此L2正则不能用于特征选择，但可以使模型更加稳定，具有更好的泛化能力。
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/0854110e-ae60-49a4-be8c-e3cb3183db85/2191d933-d4e0-400b-84b3-07dbb7d17f56/Untitled.png)
    

### 深度学习基础

- **以一层隐层的神经网络，relu激活，MSE作为损失函数推导反向传播**
    
    设训练样本集为$*D = (x(1), y(1)), (x(2), y(2)), ..., (x(n), y(n))*$，其中*x*(*i*)是输入，$*y(i)*$是对应的目标输出，*wij*表示输入层到隐层的第*i*个神经元与输出层的第*j*个神经元之间的权重，$*bj*$表示输出层的第*j*个神经元的偏置项，$*aj*$表示输出层的第$*j*$个神经元的输出，$*hi*$表示隐层的第$*i*$个神经元的输出，$*f( ⋅ )*$表示ReLU激活函数，$*L(y(i), a(i))*$表示第*i*个样本的损失函数（均方误差）。
    
    前向传播过程：
    
    首先，输入层的输出就是输入样本*x*(*i*)：
    
    $*hi = xi*$
    
    接着，隐层的输出通过线性变换计算得到：
    
    $u_j=\sum_{i=1}^{d}w_{ij}h_i+b_j$
    
    其中，$*d*$表示输入层的神经元个数，$*j = 1, 2, ..., m*$表示隐层的神经元个数。
    
    然后，隐层的输出通过ReLU激活函数进行非线性变换得到：
    
    $*f(uj) = max(0, uj)*$
    
    最后，输出层的输出通过线性变换计算得到：
    
    $a_j=\sum_{i=1}^{m}w_{ij}f(u_i)$
    
    反向传播过程：
    
    首先，计算输出层的误差项：
    
    $\delta_j=\frac{\partial L(y^{(i)},a_j)}{\partial a_j}=\frac{\partial}{\partial a_j}\frac{1}{2}(y^{(i)}-a_j)^2=-(y^{(i)}-a_j)$
    
    然后，根据链式法则计算隐层的误差项：
    
    $\delta_i=\frac{\partial L}{\partial u_i}=\frac{\partial L}{\partial a_j}\frac{\partial a_j}{\partial u_i}=\delta_jw_{ij}f'(u_i)$
    
    其中，$*f′( ⋅ )*$表示ReLU激活函数的导数：
    
    $f'(u_i)=\begin{cases}1,&u_i>0\\0,&u_i\leq0\end{cases}$
    
    接着，计算输出层到隐层之间的权重梯度：
    
    $\frac{\partial L}{\partial w_{ij}}=\frac{\partial L}{\partial a_j}\frac{\partial a_j}{\partial u_i}h_i=\delta_jf(u_i)$
    
    最后，计算输出层的偏置项梯度和隐层的偏置项梯度：
    
    $\frac{\partial L}{\partial b_j}=\delta_j$
    
- **NN的权重参数能否初始化为0？**
    - 通常情况下，不建议将神经网络的权重参数初始化为0。这是因为，如果所有的权重参数都初始化为0，那么每个神经元在前向传播时都会输出相同的结果，导致所有层的输出都相同，使得网络失去了表达能力。此外，由于反向传播过程中的梯度计算是基于权重参数进行的，如果所有的权重参数都相同，则每个权重参数在反向传播过程中都会受到相同的更新，导致权重参数无法学习。
    - 因此，在神经网络的训练中，权重参数的初始化通常需要采用一些随机的方法，例如高斯分布、均匀分布等。这样可以确保不同的神经元在前向传播时输出不同的结果，增强网络的表达能力，也可以帮助权重参数学习不同的特征，提高网络的准确性。
    - 需要注意的是，虽然不建议将所有权重参数初始化为0，但有些情况下可以将某些层的权重参数初始化为0，例如全连接层中的偏置参数可以初始化为0。这是因为偏置参数只对应一个神经元，而不像权重参数对应多个神经元，因此对于偏置参数初始化为0不会导致所有神经元的输出相同。但是，在实际应用中，通常使用其他初始化方法来初始化偏置参数，例如均匀分布、高斯分布等。
- **什么是梯度消失和梯度爆炸？**
    - 梯度消失指的是在反向传播过程中，梯度逐渐变小，最终变得非常小，无法对神经网络的权重进行有效的更新。这个问题通常出现在深度神经网络中，尤其是在使用sigmoid或tanh等S型激活函数时更为明显。因为这些函数的导数在接近0的地方变得非常小，当网络很深时，梯度不断乘以小于1的数会导致梯度趋近于0。
    - 梯度爆炸则相反，指的是在反向传播过程中，梯度逐渐变大，最终变得非常大，使得神经网络的权重值变化非常快，导致训练不稳定。梯度爆炸通常发生在循环神经网络（RNN）等结构中，因为这些结构具有很强的时序依赖性，当梯度在时间上传递时，会导致梯度指数级增长。
    - 为了解决这些问题，一些方法被提出，如ReLU激活函数的使用和批量归一化（Batch Normalization）等技术，它们可以帮助缓解梯度消失和梯度爆炸的问题。此外，还有其他技术，如梯度剪切（Gradient Clipping）等，可以限制梯度的大小，防止梯度爆炸。
- **常用的激活函数，导数？**
    1. Sigmoid 函数：
    
    Sigmoid 函数可以将实数值映射到 (0, 1) 的区间内。其公式为：
    
    $\sigma(x)=\frac{1}{1+e^{-x}}$
    
    其导数为：
    
    $\frac{\partial}{\partial x}\sigma(x)=\sigma(x)\cdot(1-\sigma(x))$
    
    1. ReLU 函数：
    
    ReLU 函数可以将负值直接置为 0，可以有效地避免梯度消失问题。其公式为：
    
    ReLU(*x*) = max (0, *x*)
    
    其导数为：
    
    $\frac{\partial}{\partial x}\text{ReLU}(x)=\begin{cases} 1 & (x>0) \ 0 & (x\leq0) \end{cases}$
    
    虽然ReLU激活函数在输入为0的点上不可导，但这在实际中并不会对神经网络的训练带来太大影响。在实现中，可以将输入为0的点的导数定义为0，从而避免对反向传播算法的影响。
    
    1. Leaky ReLU 函数：
    
    Leaky ReLU 函数是对 ReLU 函数的改进，可以避免出现 ReLU 函数中的神经元“死亡”的问题。其公式为：
    
    $\text{LeakyReLU}(x)=\begin{cases} x & (x>0) \ \alpha x & (x\leq0) \end{cases}$
    
    其中，*α* 为一个小于 1 的常数，通常取 0.01。
    
    其导数为：
    
    $\frac{\partial}{\partial x}\text{LeakyReLU}(x)=\begin{cases} 1 & (x>0) \ \alpha & (x\leq0) \end{cases}$
    
    1. Tanh 函数：
    
    Tanh 函数可以将实数值映射到 ( − 1, 1) 的区间内。其公式为：
    
    $\text{tanh}(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$
    
    其导数为：
    
    $\frac{\partial}{\partial x}\text{tanh}(x)=1-\text{tanh}^2(x)$
    
- **relu的有优点？又有什么局限性？他们的系列改进方法是啥？**
    
    ReLU（Rectified Linear Unit）是一种常用的激活函数，它具有以下优点：
    
    1. 计算速度快：ReLU函数的计算非常简单，只需要进行一次max运算即可，相比于Sigmoid和tanh等激活函数，ReLU的计算速度更快。
    2. 解决梯度消失问题：ReLU的导数在正区间恒为1，因此在正区间可以避免梯度消失问题，使得神经网络可以更快地学习。
    3. 收敛速度快：由于ReLU函数的性质，使得梯度下降算法可以更快地收敛。
    
    虽然ReLU函数具有以上优点，但也存在一些局限性：
    
    1. 死亡ReLU现象：当输入的值为负数时，ReLU函数的导数为0，称为“死亡ReLU”，此时该神经元将不再对模型产生贡献，可能会导致网络性能下降。
    2. 输出不是均值为0：ReLU的输出范围是[0,∞)，不是以0为中心的，可能会导致神经网络输出的分布不均衡。
    
    为了解决ReLU存在的问题，有许多改进方法被提出，例如：
    
    1. Leaky ReLU：在负区间，使用一个小的斜率，使得ReLU函数在负区间也有一定的导数。
    2. PReLU（Parametric ReLU）：与Leaky ReLU类似，但将斜率变成可学习的参数，通过反向传播算法优化。
    3. ELU（Exponential Linear Unit）：在负区间使用一个指数函数，使得在负区间有较小的导数。
    4. Maxout：使用多个ReLU函数取最大值，相当于学习多个ReLU函数，有更强的拟合能力。
    
    这些改进方法可以一定程度上解决ReLU存在的问题，提升神经网络的性能。
    
- **sigmoid和tanh为什么会导致梯度消失？**
    
    Sigmoid和tanh是两种常用的激活函数，它们都具有“S”形状的曲线，且取值范围都在[-1, 1]之间。在神经网络中，这两个函数通常用于激活隐藏层的输出。然而，这些函数也存在梯度消失的问题。
    
    梯度消失是指在反向传播过程中，由于梯度逐层相乘，导致低层神经元的梯度变得非常小，甚至接近于零，使得它们的参数更新几乎不起作用，从而导致训练变得非常缓慢，甚至停滞不前。这个问题在深度神经网络中尤其严重，因为它们有许多层。
    
    Sigmoid和tanh函数都具有以下特点：
    
    - 在输入非常大或非常小的时候，函数的梯度会变得非常小，接近于零，导致梯度消失的问题。
    - 函数的导数在输入接近零的时候取值最大，但是当输入偏离零点越远，导数的值就越小，最终趋近于零。
    
    这些特点使得在深度神经网络中使用sigmoid和tanh激活函数时，梯度消失的问题非常严重。为了解决这个问题，人们提出了一些新的激活函数，比如ReLU、LeakyReLU和ELU等，它们在输入较大或较小的时候都具有较大的梯度，从而缓解了梯度消失的问题。
    
- **一个隐层需要多少节点能实现包含n元输入的任意布尔函数？**
- **多个隐层实现包含n元输入的任意布尔函数，需要多少节点和网络层？**
- **dropout为何能防止过拟合？**
    - 过拟合是机器学习中常见的问题，当模型在训练数据上表现良好但在测试数据上表现较差时，就会出现过拟合。Dropout是一种防止过拟合的技术，其基本原理是在训练过程中随机丢弃一部分神经元，从而减少神经元之间的依赖关系，防止模型对训练数据的过拟合。
    - 具体来说，Dropout的实现是在每个训练批次中随机选择一些神经元不参与训练，即将其输出值设为0。这些被随机丢弃的神经元会在下一次训练中重新参与训练，从而提高模型的鲁棒性和泛化能力。
    - Dropout的作用类似于集成学习中的bagging方法，通过随机选择不同的神经元组合，从而减少模型对任意一组神经元的依赖性，进而降低模型的方差，防止模型对训练数据的过拟合。同时，Dropout也可以使模型学习到更加鲁棒的特征表示，提高模型的泛化能力。
    - 需要注意的是，Dropout只应该在训练过程中使用，在测试过程中应该关闭Dropout，保留所有的神经元，这样才能得到模型的真实预测结果。
- **dropout和 BN 在前向传播和反向传播阶段的区别？**
    
    Dropout和Batch Normalization（BN）是两种常用的正则化技术，在神经网络中被广泛使用。它们在前向传播和反向传播阶段的实现方式有所不同。
    
    在前向传播阶段，Dropout和BN的实现方式如下：
    
    - Dropout：在每个训练批次中，以一定的概率（通常为0.5）随机丢弃神经网络中的一些节点（通常是隐藏层的节点），即将它们的输出值置为零。这样可以随机地让一些神经元失活，从而减少过拟合的风险。在测试阶段，Dropout不再起作用，而是使用整个网络来进行推理。
    - BN：对于每个特征维度，计算其均值和方差，并将其标准化为零均值和单位方差的分布。然后对其进行缩放和平移，即乘以一个尺度参数和加上一个偏移参数，从而让网络学习到更好的特征表示。在训练和测试阶段都使用。
    
    在反向传播阶段，Dropout和BN的实现方式如下：
    
    - Dropout：在反向传播过程中，只传递未被丢弃的神经元的梯度，其他神经元的梯度被置为零。这相当于在每个训练批次中，以一定的概率随机地让一些神经元失活，从而减少过拟合的风险。在测试阶段，Dropout不再起作用，因此无需进行反向传播。
    - BN：在反向传播过程中，计算每个特征维度的梯度，并将其传递到上一层。BN可以看作是一种平移和缩放操作，因此需要同时更新尺度参数和偏移参数的梯度。这样可以让网络学习到更好的特征表示，从而加快收敛速度并降低过拟合的风险。
    
    综上所述，Dropout和BN在前向传播和反向传播阶段的实现方式略有不同，但它们都可以有效地防止过拟合，并提高神经网络的性能。
    

### **CNN**

- **给定卷积核的尺寸，特征图大小计算方法？**
    
    在CNN中，给定卷积核的尺寸和输入特征图的大小，可以通过以下公式计算卷积操作后得到的输出特征图的大小：
    
    $N_{out} = \left\lfloor\frac{N_{in} + 2P - K}{S}\right\rfloor + 1$
    
    其中，*Nin*是输入特征图的大小，*K*是卷积核的大小，*P*是填充的大小，*S*是步长。⌊ ⋅ ⌋表示向下取整。
    
    需要注意的是，这个公式中的卷积操作指的是不带偏置项的卷积操作。如果带有偏置项，则还需要在输出特征图的每个通道上添加相应的偏置值。
    
    例如，假设输入特征图的大小为28 × 28，卷积核的大小为3 × 3，填充的大小为1，步长为1，则根据上述公式可以计算出输出特征图的大小为：
      $N_{out} = \left\lfloor\frac{28 + 2\times 1 - 3}{1}\right\rfloor + 1 = 28$
    
    因此，输出特征图的大小为28 × 28。
    
- **网络容量计算方法**
    
    网络容量是指神经网络可以表示的模型复杂度和规模的大小，通常使用参数数量或者计算复杂度来衡量。在深度学习中，网络容量的大小通常与模型的性能和泛化能力有关，即容量过大或过小都可能导致模型的性能下降。
    
    以下是计算神经网络容量的两种常用方法：
    
    1. 参数数量：神经网络的容量可以通过统计模型中可训练参数的数量来估计。对于全连接层，可训练参数的数量等于输入维度与输出维度之积。对于卷积层，可训练参数的数量等于卷积核大小与输入通道数之积，再乘以输出通道数。对于循环神经网络，可训练参数的数量等于循环层的参数数量加上前馈层的参数数量。通过统计所有层的可训练参数数量，可以得到整个神经网络的容量大小。
    2. 计算复杂度：神经网络的容量也可以通过计算模型的浮点运算次数来估计。对于全连接层，每个神经元之间需要进行一次乘加运算，因此浮点运算次数等于输入维度与输出维度之积乘以2。对于卷积层，每个卷积核需要进行一次乘加运算，因此浮点运算次数等于卷积核大小与输入通道数之积乘以输出通道数，再乘以输出特征图的大小。对于循环神经网络，可以通过将循环层展开为多个前馈层来计算模型的浮点运算次数。通过统计所有层的浮点运算次数，可以得到整个神经网络的容量大小。
- **共享参数有什么优点**
    
    共享参数是指在神经网络中，多个神经元或多层之间使用相同的参数，以减少模型中需要学习的参数数量，从而降低过拟合的风险。
    
    共享参数的优点如下：
    
    1. 减少需要学习的参数数量，从而降低了模型的复杂度，减少了过拟合的风险。
    2. 使得模型更加紧凑和高效，从而减少了存储和计算的开销。
    3. 可以让神经网络学习到数据的某些共性特征，增加模型的泛化能力。
    4. 在训练数据不足的情况下，共享参数可以让模型更好地利用有限的数据，提高模型的性能。
    
    共享参数主要应用于卷积神经网络等模型中，通过在卷积层中共享卷积核的方式来减少模型中需要学习的参数数量。在自然语言处理等领域，也可以通过共享词向量的方式来减少模型中需要学习的参数数量，从而提高模型的效率和性能。
    
- **常用的池化操作有哪些？有什么特点？**
    
    池化（Pooling）是一种常用的神经网络层，用于减少特征图的大小和数量，从而减少计算量和内存消耗，同时还能帮助网络提取更加鲁棒的特征。常用的池化操作包括：
    
    1. 最大池化（Max Pooling）：在池化窗口内取最大值作为输出，通常用于减少特征图的空间维度。最大池化具有位置不变性和抗干扰能力强的特点，能够提取物体的主要特征。
    2. 平均池化（Average Pooling）：在池化窗口内取平均值作为输出，通常用于减少特征图的通道维度。平均池化能够提取整体特征，但在提取物体边缘等细节特征方面表现较弱。
    3. L2池化（L2 Pooling）：在池化窗口内对各个元素平方求和，再开方得到L2范数作为输出。L2池化能够提取强调低频信息的特征，但对高频信息不敏感。
    4. 重叠池化（Overlapping Pooling）：在池化窗口中按指定步长滑动，可实现池化窗口之间的重叠，从而增加特征图的采样率和覆盖范围，提高特征的丰富性。
    
    以上池化操作各有特点，在实际应用中可以根据具体任务和特征的性质选择合适的池化方式。
    
- **CNN如何用于文本分类？**
    
    卷积神经网络（Convolutional Neural Network, CNN）是一种广泛应用于图像识别和计算机视觉任务的深度学习模型。然而，CNN也可以用于文本分类任务中，具体方法如下：
    
    1. 文本表示：将文本数据转换为计算机可以处理的数值形式，通常使用词袋模型、词向量或字符级表示等方法。
    2. 卷积操作：将文本表示作为输入，在一维卷积层中进行卷积操作，从而提取局部特征。卷积层通常使用多个不同大小的卷积核，以提取不同大小的特征。
    3. 池化操作：在卷积层之后进行池化操作，通常使用最大池化或平均池化等方式，以降低维度和提高模型的鲁棒性。
    4. 全连接层：在池化层之后添加一些全连接层，以将文本特征映射到目标类别上。
    5. 激活函数和正则化：在每一层之后添加激活函数和正则化方法，以提高模型的性能和稳定性。
    6. 输出层：将最后一层的输出映射到目标类别上，通常使用softmax函数实现多分类。
    
    需要注意的是，在文本分类中，卷积核的大小应该考虑到文本序列长度和语义信息的组合，同时需要针对不同的文本分类任务选择不同的模型结构和参数设置，以提高模型的性能和泛化能力。
    
- **resnet提出的背景和核心理论是？**
    
    ResNet（Residual Network）是由微软亚洲研究院的Kaiming He等人提出的一种深度残差网络，旨在解决深度神经网络训练过程中的梯度消失和网络退化问题。
    
    在传统的神经网络中，随着网络层数的增加，梯度消失和梯度爆炸等问题逐渐加剧，导致模型性能难以进一步提升。ResNet通过引入残差连接的方式，使得网络可以跨层直接传递信息，从而避免了梯度消失和梯度爆炸问题，同时也可以加速网络的收敛和提高模型的性能。
    
    ResNet的核心理论是“残差学习”（Residual Learning）。Residual Learning认为，如果能够将网络中的非线性变换（如ReLU等）和恒等变换进行分离，将恒等变换作为一种跨层连接（Shortcut Connection），则可以使得网络可以更容易地学习到恒等变换，从而可以更好地训练深度神经网络。
    
    具体地，ResNet使用了残差块（Residual Block）来实现跨层连接。残差块由两个卷积层和一个跨层连接组成，其中跨层连接直接将输入特征图添加到输出特征图上，从而实现了跨层直接传递信息的功能。通过堆叠多个残差块，可以构建出非常深的神经网络，提高模型的性能和泛化能力。
    
    ResNet的提出，不仅在图像识别领域取得了显著的成果，同时也对深度学习理论和应用产生了重要影响。
    
- **空洞卷积是什么？有什么应用场景？**
    
    空洞卷积（Atrous Convolution或Dilated Convolution）是一种在深度神经网络中使用的卷积方式，可以增加卷积层的感受野（Receptive Field），并在保持计算量不变的情况下提高网络的性能。
    
    在传统的卷积操作中，每个卷积核的每个参数只作用于输入张量的一个像素，而空洞卷积在卷积核内部引入空洞，使得每个参数可以作用于输入张量中多个像素。具体地，空洞卷积在卷积核中插入空洞，使得卷积核在输入张量上的采样点之间留出一些空隙，从而扩大了卷积核的感受野。
    

### RNN

- **简述RNN，LSTM，GRU的区别和联系**
- **画出lstm的结构图，写出公式**
- **RNN的梯度消失问题？如何解决？**
    
    RNN（循环神经网络）的梯度消失问题是由于在反向传播时，梯度在时间步上不断地缩小，导致较早的时间步的梯度难以传递到后面的时间步，从而影响了模型的训练效果。
    
    这个问题的产生主要是因为RNN中的权重矩阵在时间步上被重复使用，导致梯度在反向传播过程中被多次乘以这些权重矩阵。当这些权重矩阵的特征值小于1时，多次相乘会使得梯度在时间步上不断缩小，最终可能会消失到几乎为0的程度。
    
    1. LSTM（长短时记忆网络）和GRU（门控循环单元）：LSTM和GRU是常用的RNN变种，它们通过引入门控机制来控制信息的流动，从而减轻了梯度消失问题。
    2. 梯度裁剪（Gradient Clipping）：在反向传播时对梯度进行裁剪，将梯度限制在一个固定范围内，从而防止梯度爆炸的同时也可以减轻梯度消失问题。
    3. 双向循环神经网络（Bidirectional RNN）：在双向循环神经网络中，一个正向的RNN和一个反向的RNN同时处理输入序列，从而可以更好地捕捉到序列中的信息。
    4. Skip Connections：在RNN中引入跨层连接，将前一层的信息直接传递给后一层，从而提高梯度的流动效率。
    5. Layer Normalization：在RNN的每一层中引入Layer Normalization，可以将每一层的输入进行归一化处理，从而使得梯度更容易传递。
    6. 梯度消失的原因往往与激活函数的选择有关，一些激活函数，如ReLU、LeakyReLU等，可以一定程度上缓解梯度消失问题。
- **lstm中是否可以用relu作为激活函数？**
    
    在LSTM中，通常使用双曲正切函数（tanh）作为激活函数，因为tanh函数的取值范围在[-1, 1]之间，可以使得LSTM的输出也在这个范围内，有利于LSTM的稳定性和收敛性。此外，tanh函数的导数也比较容易计算，方便进行反向传播算法更新模型参数。
    
    相比之下，ReLU（Rectified Linear Unit）函数在深度神经网络中更为常用，因为它可以有效地解决梯度消失问题，并且计算速度更快。但是，将ReLU函数应用于LSTM中可能会导致LSTM的输出不在[-1, 1]之间，这可能会影响模型的稳定性和收敛性。
    
- **lstm各个门分别使用什么激活函数？**
- **简述seq2seq模型？**
- **seq2seq在解码时候有哪些方法？**
    
    在seq2seq模型中，解码器需要根据编码器生成的上下文向量（context vector）和上一个时间步的输出，生成当前时间步的输出。为了提高模型的性能，可以采用以下几种方法来解码：
    
    1. 贪心解码：在每个时间步，选择输出概率最高的词作为当前时间步的输出。这种方法简单高效，但可能会导致模型陷入局部最优解。
    2. Beam Search：在每个时间步，保留输出概率最高的*k*个候选输出。然后，将这*k*个候选输出分别与下一个时间步的所有候选输出相乘，得到*k* × *V*个新的候选输出，其中*V*是词汇表的大小。再从中选择输出概率最高的*k*个候选输出，作为下一个时间步的候选输出。重复这个过程，直到生成完整个序列。这种方法可以避免贪心解码的局部最优问题，但需要手动调整*k*的大小来平衡性能和速度。
    3. Sampling：在每个时间步，根据输出概率分布随机采样一个词作为当前时间步的输出。这种方法可以增加模型的多样性，但可能会导致输出不稳定。
    4. Top-k Sampling：在每个时间步，选择输出概率最高的*k*个词作为候选输出，然后根据概率重新采样一个词作为当前时间步的输出。这种方法可以增加模型的多样性，同时避免输出不稳定的问题。
    5. Top-p Sampling（也称为Nucleus Sampling）：在每个时间步，选择输出概率累计和大于预设阈值*p*的最小*k*个词作为候选输出，然后根据概率重新采样一个词作为当前时间步的输出。这种方法可以根据概率分布自适应地调整候选输出的数量，同时增加模型的多样性。

## 计算机视觉

- **哪些方法可以提升小目标检测的效果？**
    1. 使用更好的特征提取器：小目标的特征通常很微弱，需要使用更好的特征提取器来提取更丰富的特征。一些常见的特征提取器包括卷积神经网络（CNN）、深度残差网络（ResNet）等。
1. 增加训练数据：更多的训练数据可以提高模型的准确性和泛化性能。对于小目标检测来说，尤其需要大量的训练数据来覆盖各种场景和目标形状。
    1. 使用多尺度检测：在不同的尺度上检测目标可以提高检测率和准确率。可以使用多尺度图像金字塔或多尺度特征图来检测小目标。
2. 使用注意力机制：注意力机制可以使模型更加关注小目标的区域，以提高检测准确率。可以使用空间注意力机制或通道注意力机制来实现。
    1. 数据增强：数据增强可以使训练数据更加多样化，以提高模型的泛化能力。可以使用随机裁剪、旋转、平移等方法来增加数据。
3. 使用后处理技术：后处理技术可以进一步提高模型的准确率和稳定性。例如，可以使用非极大值抑制（NMS）来去除重叠框或使用边框回归来精确定位目标。
    1. 使用更好的评估指标：常用的评估指标，如mAP（mean Average Precision），可能不适用于小目标检测。可以使用更适合小目标检测的指标，如F1-Score，IoU等。
- **如果模型要部署在边缘设备上并且运行速度很慢，有几种方法可以尝试提高运行速度？**
    1. 对模型进行剪枝，即减少模型的参数数量，使模型更小并使运行速度更快。
    2. 使用低精度运算，例如使用 16 或 8 位浮点数进行运算，而不是 32 或 64 位浮点数。
    3. 将模型转化为更加高效的格式，例如使用 TensorFlow Lite 或 ONNX Runtime 等工具将模型转化为更加高效的格式。
    4. 对模型进行重新设计，使用更加高效的模型架构，例如 MobileNet 或 ShuffleNet。
    5. 使用更加高效的边缘设备，例如使用 GPU 或 ASIC 等较快的处理器。
- **模型检测准确性不高可以从哪些方面考虑改进？**
    
    获取更多的训练数据，可以帮助模型更好地泛化到新的数据。
    
    对数据进行进一步的预处理和增强，例如随机翻转或旋转图像，或者在图像上添加噪声。
    
    调整模型的超参数，例如学习率、正则化系数等。
    
    使用更大的模型，例如更多的层数或更多的单元。
    
    尝试使用不同的模型架构，例如使用更深的卷积神经网络或使用更复杂的循环神经网络。
    
    尝试使用转换学习，即使用已经训练好的模型来提取特征，并使用这些特征来训练新的模型。
    
- **要做一个车辆检测和计数功能的模型，考虑具体怎么实现?**
    1. 准备训练数据：首先，需要准备一些包含车辆的图像作为训练数据。这些图像应该是真实的街景图像，并且应该包含不同 的车辆数量和种类。
    2. 训练目标检测模型：其次，使用这些图像训练目标检测模型，例如 YOLO 或 SSD。这些模型能够在图像中检测到车辆并输出车辆的边界框。
    3. 训练计数模型：然后，使用训练好的目标检测模型在图像中检测到的车辆边界框来训练计数模型。计数模型应该能够根据输入的车辆边界框来输出车辆的数量。
    4. 部署模型：最后，将训练好的目标检测模型和计数模型部署到目标设备上，使用摄像头获取实时图像，然后使用模型检测和计数车辆。
- **DW卷积 和 PW卷积**
    
    DW卷积（Depthwise Convolution）是指对每个输入通道分别进行卷积操作，即对于每个输入通道，使用一个不同的卷积核进行卷积运算。DW卷积通常用于减少模型的计算量和参数数量，特别是在移动设备和嵌入式系统中，可以在不损失太多模型性能的情况下减小模型的体积和计算复杂度。
    
    PW卷积（Pointwise Convolution）是指使用一个 1 × 1 的卷积核对输入进行卷积操作。这种卷积操作在卷积核大小为 1 × 1 时等价于全连接层，因此也被称为“1x1卷积”。PW卷积通常用于调整输入张量的通道数和维度，以及引入非线性变换和特征提取。
    
    通常，DW卷积和PW卷积可以结合使用，以构建轻量级的深度卷积神经网络。例如，MobileNet就是使用DW卷积和PW卷积构建的一种轻量级卷积神经网络，具有较小的模型体积和较低的计算复杂度，适用于移动设备和嵌入式系统
    
- **SPP在YOLO的作用，有何优缺点？**
    
    Spp在YOLO中是融合局部和整体特征.通过SPP模块实现了局部特征和全局特征，这也是为什么SPP模块中最大的池化核大小要尽可能的接近或者等于需要池化的特征图的大小，特征图经过局部特征与全局特征相融合后，丰富了特征图的表达能力，有利于待检测图像中目标大小差异较大的情况，尤其是对于YOLOv3这种复杂的多目标检测，所以对检测的精度上有了很大的提升。
    
    1. SPP 的优点： 简单性：SPP 是一种简单的技术，可以很容易地添加到现有的 CNN 架构中，从而在不增加太多复杂性的情况下提高网络的性能。 多尺度表示：SPP 允许模型从多个尺度捕获信息，使其对对象大小的变化具有鲁棒性。 固定大小表示：SPP 提供了特征图的固定大小表示，这使得它更容易与最终分类器集成。
    2. SPP 的缺点：有限的表示：SPP 只考虑预定义尺度的池化，这可能并不适合所有对象。 高计算成本：由于多个池化操作和特征图的连接，SPP 会增加大量的计算开销。
- **FPN网络在目标检测领域的作用，有何优缺点？**
    
    FPN的思路剑指小目标，原来很多目标检测算法都是只采用高层特征进行预测，高层的特征中语义信息比较丰富，但是分辨率较低，目标位置比较粗略。假设在深层网络中，最后的高层特征图中一个像素可能对应着输出图像 的像素区域，那么小于 像素的小物体的特征大概率已经丢失。与此同时，低层的特征语义信息比较少，但是目标位置准确,这是对小目标检测有帮助的。FPN将高层特征与底层特征进行融合，从而同时利用低层特征的高分辨率和高层特征的丰富语义信息，并进行了多尺度特征的独立预测，对小物体的检测效果有明显的提升。
    
    1. FPN的优点：多尺度表示：和SPP一样，FPN允许模型从多个尺度上捕捉信息，提高了检测不同大小物体的能力。 高级语义信息：FPN 结合了来自深层的高级语义信息，这对于检测具有复杂形状或纹理的对象很有用。 低级精细细节：FPN 还结合了浅层的低级精细细节，这对于检测具有小细节或精细细节的物体很有用。
    2. FPN 的缺点： 复杂性：与 SPP 相比，FPN 是一种更复杂的技术，需要对底层架构有更深入的了解才能正确实施。 计算使用效率低下：FPN 有时计算量大，尤其是与大型 CNN 结合使用时。
- **多尺度训练的细节**
    
    多尺度训练是深度学习中的一种技术，涉及在同一图像或数据的多个尺度上训练模型。 这通常是通过将输入图像调整为不同比例（例如 0.5 倍到 1.5 倍）并在每个缩放版本上训练模型来实现的。 这项技术背后的想法是，它可以帮助模型更好地处理尺度变化，因为它在训练过程中以多种尺寸暴露于相同的物体或特征。 多尺度训练已被证明可有效提高目标检测和图像分类模型的准确性和鲁棒性。
    
- **混合精度**
    
    混合精度是指在训练和推理过程中，对深度学习模型的不同部分使用不同的数值精度（例如半精度和单精度）。 这可以减少内存使用和计算时间，从而允许在可用硬件上运行更大、更复杂的模型。 它还可以通过对模型的关键部分使用更高的精度和对不太重要的部分使用较低的精度来提高准确性。
    
- **预热和 Cosine LR 调度程序**
    
    热身是深度学习中的一种技术，用于在训练开始时通过几个训练步骤慢慢提高学习率，以防止优化器被高梯度淹没。 余弦 LR 调度器是一种学习率调度方法，它使用余弦函数随时间调整学习率。 学习率从一个高值开始，并在训练过程中逐渐降低，最终达到零。
    
    这种方法基于最佳学习率在训练过程中下降的观察结果，它提供了从高学习率到低学习率的平滑过渡。 warmup和cosine LR调度器都用于控制训练时的学习率，目的是提高优化过程的稳定性和收敛性。 预热帮助优化器逐渐适应训练过程，而余弦 LR 调度器提供学习率随时间的平滑衰减。
    
- **细粒度分类**
    
    细粒度分类（Fine-grained classification）是指在分类任务中，需要对物体或图像的细节部分进行精细分类，例如对不同品种的狗进行分类，或者对不同类型的花进行分类。相对于传统的分类任务，细粒度分类更加困难，因为细节的差异往往非常微小，需要更高的准确性和敏感性。
    
    深度学习是实现细粒度分类的有效方法之一，它可以利用卷积神经网络（CNN）来学习图像的特征，并将这些特征与相应的标签关联起来。深度学习模型通常需要训练大量的图像数据，以便在学习过程中捕捉到物体的不同特征和细节，并且在测试阶段可以准确地识别和分类新的图像。
    
    在细粒度分类中，还存在一些挑战。其中一个主要挑战是数据的标注。由于需要对图像的细节部分进行分类，标注者需要具有专业知识，才能准确地区分物体的差异。此外，由于不同物体之间的差异非常微小，深度学习模型很容易受到数据中的噪声和不一致性的影响，因此需要采用一些有效的数据清洗和预处理方法。
    
- **Batch Normalization**
    
    Batch Normalization (BN) 是一种深度学习中常用的正则化方法，旨在加速神经网络的训练过程。其思想是在训练过程中对每个小批量数据进行归一化处理，即通过减去批量数据的均值并除以其标准差，从而使得输入的数据服从标准正态分布，从而加速神经网络的训练。
    
    BN 层的具体实现方式为：对于输入的每一个数据样本，先减去均值，再除以方差加一个小数，再乘以一个缩放系数，最后再加上一个平移系数。
    
    Batch Normalization的优点是可以减少梯度消失问题，同时也可以加速训练速度，提高网络的泛化能力。然而，Batch Normalization也存在一些缺点，例如在小批量数据上可能会产生较大的误差，而且需要额外的计算代价和内存开销。
    
- **Drop out**
    
    是深度学习中的一种正则化技术，用于通过在前向和反向传播的每次迭代期间随机丢弃（即设置为零）一些神经元来防止过度拟合。 这迫使模型学习输入数据的多个独立表示，并可能产生更具通用性的模型。 在训练过程中，每次迭代都会随机丢弃一定比例的神经元，这有助于防止过度拟合。 在测试期间，所有神经元都用于进行预测，目的是评估模型在未见数据上的整体性能。
    
- **多卡训练的细节**
    
    多卡训练是一种用于跨多个 GPU 并行化深度学习模型训练过程的技术。 目标是加快训练过程，减少训练时间。
    
    多卡训练的细节可能因所使用的框架而异，但以下是一些常见的注意事项： 数据并行：这涉及将小批量数据拆分到多个 GPU 并并行计算梯度。 模型并行性：这涉及将模型拆分到多个 GPU 上，其中每个 GPU 都保存模型参数的一部分。 同步：每个 GPU 计算的梯度需要同步和平均以获得模型参数的单一更新。 这通常是通过诸如 all- reduce 或参数服务器之类的机制来完成的。 负载平衡：确保每个 GPU 执行的工作量大致相等非常重要，因为这有助于确保训练过程高效。 通信开销：GPU 之间的通信可能是多卡训练中的瓶颈，将这种开销降至最低以实现最佳性能非常重要。
    
    值得注意的是，多卡训练并不总是一个简单的过程，需要仔细考虑模型架构、数据并行策略和硬件配置才能取得良好的效果。
    
- **显存不足有什么办法**
    
    减少批量大小：训练具有大批量大小的深度学习模型会消耗大量 GPU 内存。 尝试将批量大小减小到较小的值，看看是否有帮助。
    
    减小模型大小：具有更多参数的较大模型需要更多内存来存储它们的权重。 您可以尝试通过使用参数较少的架构或修剪技术来删除冗余参数来减小模型的大小。
    
    梯度累积：不是一次处理整个小批量，你可以累积多个小批量的梯度并在最后更新一次模型参数。这有助于减少每次迭代的内存消耗。
    
    模型并行性：如果模型太大而无法在单个 GPU 上运行，您可以将模型拆分到多个 GPU 上并使用模型并行性来训练它。
    
    使用混合精度：使用 float16 而不是 float32 进行训练可以减少一半的内存消耗，但也可能会降低模型的精度。 减少序列长度：如果您正在为语言模型或语音识别等序列训练模型，请尝试减少序列长度，这有助于减少内存消耗。
    

切换到具有更多内存的 GPU

- **anchor-based和anchor-free的方法分别有什么优缺点 ?**
    
    Anchor-based和anchor-free是计算机视觉中的两种目标检测方法。 . anchor-based的方法，例如 Faster R-CNN，使用预定义的锚框来检测对象。 锚框用作识别对象位置和形状的参考。 基于anchor的方法的优点是可以很好地处理不同纵横比的对象，而且速度相对较快。 然而，它们可能不太准确并且难以检测小物体，因为锚框可能无法很好地适应物体。 Anchor-free 方法，例如 YOLO 和 RetinaNet，不使用预定义的锚框。 相反，他们直接从特征图中预测对象边界框和类别概率。 anchor-free 方法的优点是更灵活，可以更准确，特别是对于小物体。 然而，与基于锚点的方法相比，它们可能更慢并且难以检测具有不同纵横比的对象。 总之，anchor-based 和 anchor-free 方法之间的选择取决于准确性、速度和图像中对象的特征之间的权衡
    
- **重参数化网络(yolov6)**
    
    training时使用多分支的结构，论文中通过实验表明，多分支的结构有利于增加网络的表征能力，而推理的时候使用融合的结构，可以减少参数量，内存等，而且在推理的时候可以达到和多分支一样的效果。
    
- **yolov7中的创新点 E-ELAN， MP-conv，正样本的分类(yolo_v5的粗略筛选和yolox的SimOTA)**
- **从损失函数角度，如何解决类别不均衡问题?**
    
    类别不均衡问题指的是在训练数据集中，不同类别的样本数量存在较大差异，这样就可能会导致模型过度关注数量较多的类别，从而影响模型的性能。
    
    在解决类别不均衡问题时，从损失函数的角度，可以采用以下几种方法：
    
    加权损失函数：对于数量较少的类别，增加其损失函数的权重，从而使得模型更加关注这些样本。常见的加权损失函数有 Focal Loss、Weighted Cross-Entropy Loss 等。
    
    采用样本重采样：通过对数据集进行重采样，即增加数量较少的类别样本的数量，或减少数量较多的类别样本的数量，来达到类别平衡的目的。常见的方法有过采样、欠采样等。
    
- **Batch大小的选择和影响?**
    
    在不考虑Batch Normalization的情况下，batch size的大小决定了深度学习训练过程中的完成每个epoch所需的时间和每次迭代(iteration)之间梯度的平滑程度。
    
    由于目前主流深度学习框架处理mini-batch的反向传播时，默认都是先将每个mini-batch中每个instance得到的loss平均化之后再反求梯度，也就是说每次反向传播的梯度是对mini-batch中每个instance的梯度平均之后的结果，所以b的大小决定了相邻迭代之间的梯度平滑程度，b太小，相邻mini-batch间的差异相对过大，那么相邻两次迭代的梯度震荡情况会比较严重，不利于收敛；b越大，相邻mini-batch间的差异相对越小，虽然梯度震荡情况会比较小，一定程度上利于模型收敛，但如果b极端大，相邻mini-batch间的差异过小，相邻两个mini-batch的梯度没有区别了，整个训练过程就是沿着一个方向蹭蹭蹭往下走，很容易陷入到局部最小值出不来
    
- **为什么要做模型量化？**
    1. 简而言之，所谓的模型量化就是将浮点存储（运算）转换为整型存储（运算）的一种模型压缩技术。**简单直白点讲，即原来表示一个权重需要使用float32表示，量化后只需要使用int8来表示就可以啦，仅仅这一个操作，我们就可以获得接近4倍的网络加速**！
    2. 随着深度学习技术在多个领域的快速应用，具体包括计算机视觉-CV、自然语言处理-NLP、语音等，出现了大量的基于深度学习的网络模型。这些模型都有一个特点，即大而复杂、适合在N卡上面进行推理，并不适合应用在手机等嵌入式设备中，而客户们通常需要将这些复杂的模型部署在一些低成本的嵌入式设备中，因而这就产生了一个矛盾。**为了很好的解决这个矛盾，模型量化应运而生，它可以在损失少量精度的前提下对模型进行压缩，使得将这些复杂的模型应用到手机、机器人等嵌入式终端中变成了可能**。 **随着模型预测越来越准确，网络越来越深，神经网络消耗的内存大小成为一个核心的问题，尤其是在移动设备上**。通常情况下，目前的手机一般配备 4GB 内存来支持多个应用程序的同时运行，而三个模型运行一次通常就要占用1GB内存。 **模型大小不仅是内存容量问题，也是内存带宽问题**。模型在每次预测时都会使用模型的权重，图像相关的应用程序通常需要实时处理数据，这意味着至少 30 FPS。因此，如果部署相对较小的 ResNet-50 网络来分类，运行网络模型就需要 3GB/s 的内存带宽。网络运行时，内存，CPU 和电池会都在飞速消耗，我们无法为了让设备变得智能一点点就负担如此昂贵的代价。
- Faster-Rcnn
    - 为什么fastrcnn提出anchor box?
        
        Faster R-CNN模型提出了anchor box的主要原因是在目标检测任务中，不同目标之间的尺寸和比例存在较大的差异。为了解决这个问题，Faster R-CNN提出了anchor box的概念，即在图像中生成一系列不同大小和宽高比的锚框，这些锚框作为目标检测的候选框。
        
        通过使用anchor box，Faster R-CNN模型可以检测不同大小和比例的目标，并将它们与相应的锚框进行匹配。这样可以有效地减少检测误差，提高模型的准确性。
        
        具体地说，Faster R-CNN中的anchor box是指预定义的一组固定大小和宽高比的矩形框，这些框可以在图像中移动和缩放。然后，Faster R-CNN使用卷积神经网络来对每个锚框进行分类和回归，以确定哪些锚框包含了目标物体。
        
        通过使用anchor box，Faster R-CNN可以在保持高检测精度的同时提高检测速度。因为锚框是预定义的，所以模型只需要在锚框周围的区域进行计算，而不需要对整个图像进行处理。这样可以大大减少计算量和计算时间，从而提高模型的速度。
        
- YOLO_V1
- YOLO_V2
- YOLO_V3
    - **为什么Softmax不适用于多标签分类？**
        
        Softmax函数主要用于多类别分类问题，它将一组任意实数作为输入，将它们映射成一个概率分布，其中每个输出的概率代表该输入属于不同类别的概率，所有输出概率之和为1。
        
        在多标签分类问题中，每个样本可以属于多个类别，也就是每个样本可能有多个正确的标签。与多类别分类不同，多标签分类需要对每个标签分别预测一个概率值。因此，对于每个标签，我们需要一个独立的输出值，该值表示该样本属于该标签的概率。
        
        Softmax函数在多标签分类问题中不适用的主要原因是它的输出总和必须为1。这就意味着，如果一个样本属于多个标签，则所有这些标签的输出概率之和必须等于1。但在多标签分类中，每个标签应该有它独立的输出值，不应该受到其他标签的影响。
        
        因此，在多标签分类问题中，常见的解决方案是使用Sigmoid函数作为激活函数，而不是使用Softmax函数。Sigmoid函数可以将任意实数映射到一个0到1之间的值，它独立地对每个标签进行分类，因此可以为每个标签预测一个独立的概率值。这种方法通常被称为多标签二分类或多标签逻辑回归。
        
- YOLO_V4
1. CSP模块是一种特殊的卷积模块，它将输入特征分为两个部分，一部分通过卷积层进行处理，另一部分则不做处理。处理后的特征和未处理的特征进行拼接后再通过卷积层输出。这种方式能够使得模型在不增加计算量的情况下增加网络深度，从而提高模型的准确性.
2. CSP模块还可以解决梯度信息重复的问题。在深度神经网络中，梯度信息在反向传播时会被多次传递。这些重复的梯度信息可能会导致梯度爆炸或梯度消失的问题。CSP模块通过将特征图分成两个部分，使得梯度信息只需要在主干网络和子网络之间传递一次，从而避免了梯度信息重复的问题。
- YOLO_V5
1. Focus模块在v5中是图片进入backbone前，对图片进行切片操作，具体操作是在一张图片中每隔一个像素拿到一个值，类似于邻近下采样，这样就拿到了四张图片，四张图片互补，长的差不多，但是没有信息丢失，这样一来，将W、H信息就集中到了通道空间，输入通道扩充了4倍，即拼接起来的图片相对于原先的RGB三通道模式变成了12个通道，最后将得到的新图片再经过卷积操作，最终得到了没有信息丢失情况下的二倍下采样特征图。
2. yolov5 中 C3 模块的结构参考了 CSP 架构。 CSP 架构将特征图以通道为依据拆分为两部分， 其中一部分与传出的特征图相结合， 另一部分通过密集块和过渡 层， 使其结构具有减少计算量的同时提高网络检测性能的作用。
3. SPP利用卷积核为[1x1、 5x5、 9x9、 13x13]的最大池化方式， 堆叠尺度不同的特征图。 在研究中发现， SPP 模块相较于普通的最大池化方式， SPP 使感受野以及主干特征信息的接收范围变大， 并且能够分离更重要的上下文特征信息。
4. Yolo_v5的训练策略包括了混合精度训练和模型EMA

```
+ 混合精度训练是一种通过利用浮点数位数较小的数值格式（如半精度浮点数）来加速神经网络训练的技术。混合精度训练可以将神经网络训练中的矩阵乘法和卷积运算等计算量较大的操作从单精度浮点数格式转换为半精度浮点数格式，从而减少计算时间和内存带宽的开销，提高训练速度。

    在混合精度训练中，网络的权重和梯度通常存储在单精度浮点数中，而输入数据和中间特征图则存储在半精度浮点数中。这样可以在减少存储空间的同时，保留足够的精度以避免过多的计算误差。在计算过程中，半精度浮点数格式能够使用更小的存储空间和更快的计算速度，从而减少计算时间和内存带宽的开销.

+ MA指的是Exponential Moving Average，即指数移动平均，是一种常见的平滑算法，常用于时间序列数据的分析和处理。在深度学习中，EMA策略可以用来平滑模型参数的更新过程，从而提高模型的鲁棒性和泛化能力。

    具体来说，EMA策略可以用于模型在训练过程中的参数更新。在训练过程中，模型的参数会根据梯度下降算法进行更新。在EMA策略中，模型的参数更新不仅会考虑当前的梯度，还会考虑历史参数的平均值。具体地，EMA策略通过维护一个移动平均值来平滑模型参数的更新过程。移动平均值可以被看作是历史参数的加权平均值，其中权重系数是指数衰减的。EMA策略通过这种方式来降低参数更新的方差，并且使得模型更加稳定和鲁棒
```

- YOLOx
    
    ```python
    SiLU'(x) = sigmoid(x) + x * sigmoid(x) * (1 - sigmoid(x))         = sigmoid(x) * (1 + (x - SiLU(x)))
    ```
    
- YOLO_V6
    
    YOLOv4中的 CIoU Loss虽然考虑到检测框与 ground truth 之间的重叠面积、中心点距离，长宽比这三大因素，但是依然缺少了对检测框与ground truth之间方向的匹配性的考虑。 SIoU Loss 通过引入了所需回归之间的向量角度，重新定义了距离损失，有效降低了回归的自由度，加快网络收敛， 进一步提升了回归精度。
    
- YOLO_V7
    
    **E-ELAN** :通过控制最短最长的梯度路径，更深的网络可以有效地学习和收敛。作者提出ELAN结构。基于ELAN设计的E-ELAN 用expand、shuffle、merge cardinality来实现在不破坏原有梯度路径的情况下不断增强网络学习能力的能力。
    
- **IOU 计算**
    
    ```python
    def iou(box1, box2):    cx1, cy1, w1, h1 = box1    cx2, cy2, w2, h2 = box2    x1 = cx1 - (w1 / 2)    y1 = cy1 - (h1 / 2)    x2 = cx2 - (w2 / 2)    y2 = cy2 - (h2 / 2)    # determine the coordinates of the intersection rectangle    x_left = max(x1, x2)    y_top = max(y1, y2)    x_right = min(x1 + w1, x2 + w2)    y_bottom = min(y1 + h1, y2 + h2)    if x_right < x_left or y_bottom < y_top:        return 0.0    # The intersection of two axis-aligned bounding boxes is always an    # axis-aligned bounding box    intersection_area = (x_right - x_left) * (y_bottom - y_top)    # compute the area of both AABBs    bb1_area = w1 * h1    bb2_area = w2 * h2    # compute the union by taking the union minus the intersection    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)    return iou
    ```
    
- **计算 2D卷积**
    
    ```python
    import numpy as npdef conv2d(inputs, kernel, bias, stride=1, padding=0):    # Get the dimensions of the input and kernel    (batch_size, input_height, input_width, input_channels) = inputs.shape    (kernel_height, kernel_width, input_channels, output_channels) = kernel.shape    # Calculate the output dimensions    output_height = (input_height - kernel_height + 2 * padding) // stride + 1    output_width = (input_width - kernel_width + 2 * padding) // stride + 1    # Initialize the output feature map    outputs = np.zeros((batch_size, output_height, output_width, output_channels))    # Slide the kernel over the input feature map    for i in range(output_height):        for j in range(output_width):            for k in range(output_channels):                # Get the current slice of the input feature map                inputs_slice = inputs[:, i*stride:i*stride+kernel_height, j*stride:j*stride+kernel_width, :]                # Apply the kernel to the slice and add the bias                outputs[:, i, j, k] = np.sum(inputs_slice * kernel[:,:,:,k], axis=(1,2,3)) + bias[k]    return outputs
    ```
    
- **计算K-means**
    
    ```python
    def kmeans(X, k):    m, n = X.shape    centroids = X[np.random.choice(m, k, replace=False), :]    prev_centroids = np.zeros((k, n))    distances = np.zeros((m, k))    while np.sum(centroids != prev_centroids) != 0:        prev_centroids = np.copy(centroids)        distances = cdist(X, centroids)        labels = np.argmin(distances, axis=1)        for i in range(k):            centroids[i, :] = np.mean(X[labels == i, :], axis=0)    return centroids, labels
    ```
    
- **计算非极大值抑制**
    
    ```python
    def py_nms(dets, thresh):    """Pure Python NMS baseline."""    #x1、y1、x2、y2、以及score赋值    x1 = dets[:, 0]    y1 = dets[:, 1]    x2 = dets[:, 2]    y2 = dets[:, 3]    scores = dets[:, 4]    #每一个候选框的面积    areas = (x2 - x1 + 1) * (y2 - y1 + 1)    #order是按照score降序排序的    order = scores.argsort()[::-1] #[1, 3, 0, 2]    keep = []    while order.size > 0:        i = order[0] # 1        keep.append(i)        #计算当前概率最大矩形框与其他矩形框的相交框的坐标，会用到numpy的broadcast机制，得到的是向量        xx1 = np.maximum(x1[i], x1[order[1:]])        yy1 = np.maximum(y1[i], y1[order[1:]])        xx2 = np.minimum(x2[i], x2[order[1:]])        yy2 = np.minimum(y2[i], y2[order[1:]])        #计算相交框的面积,注意矩形框不相交时w或h算出来会是负数，用0代替        w = np.maximum(0.0, xx2 - xx1 + 1)        h = np.maximum(0.0, yy2 - yy1 + 1)        inter = w * h        #计算重叠度IOU：重叠面积/（面积1+面积2-重叠面积）        ovr = inter / (areas[i] + areas[order[1:]] - inter)        #找到重叠度不高于阈值的矩形框索引        inds = np.where(ovr <= thresh)[0]        #将order序列更新，由于前面得到的矩形框索引要比矩形框在原order序列中的索引小1，所以要把这个1加回来        order = order[inds + 1]    return keep
    ```
    

## 机器学习

### 基础

- **样本不均衡如何处理？**
    
    样本不均衡指的是在分类问题中，不同类别的样本数量存在显著差异，这可能导致模型偏向于预测数量较多的类别，而忽略数量较少的类别。处理样本不均衡的方法主要包括以下几种：
    
    1. 重采样（Resampling）：通过增加少数类样本或减少多数类样本来使不同类别的样本数量相对均衡。重采样的方法主要包括上采样（Oversampling）和下采样（Undersampling）两种。上采样常用的方法有SMOTE（Synthetic Minority Over-sampling Technique），通过生成新的少数类样本来扩充样本集；下采样常用的方法有随机欠采样（Random Undersampling）和集群中心欠采样（Cluster Centroids Undersampling），通过删除多数类样本或者减少多数类样本的数量来缩小样本集。
    2. 加权（Weighting）：对不同类别的样本赋予不同的权重，使得少数类样本在训练过程中起到更大的作用。加权的方法通常是在损失函数中加入一个权重系数，使得少数类样本在计算损失时被赋予更大的权重。
    3. 生成新特征（Feature Engineering）：通过从原始特征中生成新的特征来提高模型性能。例如，在图像分类问题中，可以通过旋转、翻转、缩放等方式生成新的图像，并将其加入到训练集中。
    4. 集成学习（Ensemble Learning）：通过组合多个模型的预测结果来提高模型性能。例如，可以使用Bagging方法来训练多个基分类器，然后对其预测结果进行投票或求平均值来得到最终的预测结果。
    5. 改变阈值（Threshold）：对模型输出的概率或得分设置不同的阈值来改变分类结果。例如，可以将分类阈值从0.5调整为0.3，使得模型更倾向于将样本分为少数类。
    
    需要根据具体的问题和数据情况选择合适的方法来处理样本不均衡问题。
    
- **什么是生成模型什么是判别模型？**
    1. 生成模型（Generative Model）是指一种建模方法，通过对输入和输出之间的关系进行建模，从而推断出输入的概率分布以及由该分布产生的输出。生成模型的目标是建立一个能够生成与实际数据相似的数据集的模型，也可以用于数据的降维、生成样本、异常检测等任务。常见的生成模型包括朴素贝叶斯、高斯混合模型（GMM）和变分自编码器（VAE）等。
    2. 判别模型（Discriminative Model）是指一种建模方法，它直接对输入和输出之间的关系进行建模，目标是建立一个能够判别输入与输出之间关系的模型，常用于分类和回归等任务。判别模型主要关注对输出的预测，而不关注输入的分布情况。常见的判别模型包括线性回归、逻辑回归、支持向量机（SVM）和深度神经网络等。
    3. 通常来说，生成模型和判别模型是针对不同类型的问题而设计的。如果我们关注的是数据生成的过程，例如图像生成、语音合成等问题，那么我们可以使用生成模型；如果我们关注的是输入与输出之间的映射关系，例如分类、回归等问题，那么我们可以使用判别模型。在实际应用中，需要根据具体问题的特点来选择适合的模型类型。

### 集成学习

- **集成学习的分类？有什么代表性的模型和方法？**
    
    集成学习（Ensemble Learning）是一种通过将多个分类器组合来提高预测准确性的机器学习方法。常见的集成学习分类有以下几种：
    
    1. Boosting：Boosting 是一种序列化集成学习方法，其中每个模型都尝试修正其前一个模型的错误。代表性的模型包括 AdaBoost、Gradient Boosting 和 XGBoost。
    2. Bagging：Bagging 是一种并行集成学习方法，其中每个模型都是使用随机抽取的样本进行训练。代表性的模型包括随机森林（Random Forest）和 ExtraTrees。
    3. Stacking：Stacking 是一种层级集成学习方法，其中第一层使用多个模型进行预测，并将它们的输出作为输入用于训练第二层模型。代表性的模型包括神经网络。
    4. Blending：Blending 与 Stacking 类似，但不需要使用第二层模型进行整合。它将不同模型的预测结果简单加权平均。
    5. 场景集成学习：场景集成学习将数据分为多个场景，并在每个场景中使用不同的模型。代表性的方法包括异构集成学习（Heterogeneous Ensemble）和多任务学习（Multi-Task Learning）。
    
    总体来说，Boosting 和 Bagging 是最常用的集成学习方法，而 Stacking 和 Blending 则适用于更高级的问题，例如推荐系统和图像识别。
    
- **如何从偏差和方差的角度解释bagging和boosting的原理？**
    
    它们都是基于模型集成的思想，通过组合多个弱分类器来构建一个强分类器。下面从偏差和方差的角度解释这两种方法的原理：
    
    1. Bagging（自举汇聚法）
    
    Bagging的基本思想是通过自助采样法（bootstrap sampling）从原始训练数据集中抽取多个不同的子样本，对每个子样本独立地训练一个弱分类器，然后将多个弱分类器组合成一个强分类器。通过平均化多个分类器的预测结果，可以降低模型的方差，提高模型的泛化能力。
    
    从偏差和方差的角度来看，Bagging主要起到了降低模型方差的作用。由于每个弱分类器只是在子样本上进行训练，可能会忽略部分训练数据，导致模型存在一定的偏差。但是，由于通过自举采样得到的不同子样本之间具有较强的独立性，多个弱分类器的预测结果可以互相抵消，从而有效地减小了模型的方差，提高了模型的泛化能力。
    
    1. Boosting（提升法）
    
    Boosting的基本思想是通过迭代地训练弱分类器，每一轮训练时，根据前一轮训练的结果调整训练数据的权重，使得模型更加关注分类错误的样本，从而不断提高模型的性能。每个弱分类器都是针对前一轮训练的误差进行训练的，通过加权结合多个弱分类器的预测结果，可以得到一个强分类器。
    
    从偏差和方差的角度来看，Boosting主要起到了降低模型偏差的作用。由于Boosting会针对分类错误的样本进行训练，从而更加关注难以分类的数据，可以有效地降低模型的偏差，提高模型的准确率。但是，在Boosting的过程中，多个弱分类器的预测结果可能会出现比较大的波动，导致模型的方差较高。因此，为了平衡偏差和方差，Boosting通常采用一些正则化方法，如加权平均、剪枝等来提高模型的泛化能力。
    
- **GBDT的原理？和Xgboost的区别联系？**
    
    GBDT（Gradient Boosting Decision Tree）是一种基于决策树的集成学习算法，它通过迭代地训练一系列的决策树模型，并通过加权求和的方式进行集成。每次迭代都尝试纠正之前模型的预测误差，从而得到更加准确的预测结果。
    
    具体来说，GBDT算法的原理如下：
    
    1. 初始化模型：使用一个简单的模型（例如平均值）来初始化模型。
    2. 迭代训练：在每一轮迭代中，GBDT通过以下步骤来训练一个新的决策树模型：
        1. 计算残差：将当前模型的预测值与真实值相减，得到残差。
        2. 训练决策树：使用残差作为目标变量，训练一个新的决策树模型。
        3. 更新模型：将新的决策树模型与之前的模型进行加权相加，得到新的模型。
    3. 输出模型：最终输出迭代训练得到的所有决策树模型的加权和作为最终模型的预测结果。
    
    而Xgboost（eXtreme Gradient Boosting）则是在GBDT的基础上进行改进和优化的算法，具体区别如下：
    
    1. 对损失函数进行了改进，采用了二阶泰勒展开式来拟合损失函数，使得Xgboost可以同时处理分类和回归问题。
    2. 引入了正则化项，控制模型的复杂度，避免过拟合。
    3. 增加了对缺失值的处理能力，可以自动学习出缺失值所对应的分裂方向。
    4. 改进了GBDT中的决策树生成算法，引入了直方图算法，将连续的特征离散化成桶，减少了决策树生成的时间。
    5. 支持并行计算，在分布式环境下，可以利用多台计算机加速模型训练。
    
    总之，Xgboost是在GBDT的基础上进行了改进和优化，加入了更多的技巧，使得它在精度和效率上都有很大的提升，是一种性能非常优秀的集成学习算法。
    
- **adaboost和gbdt的区别联系？**
    
    AdaBoost（Adaptive Boosting）和GBDT（Gradient Boosting Decision Tree）都是常见的集成学习算法，但它们在很多方面有着不同的思想和实现方法。
    
    1. 基本思想：AdaBoost是一种迭代算法，每轮迭代训练一个弱分类器，并根据分类器的准确率来调整样本权重，使得错误分类的样本在下一轮迭代中得到更多的关注。GBDT也是一种迭代算法，但每轮迭代训练的是一个回归树模型，每个模型的训练目标是拟合上一轮模型的残差，最终将所有模型的输出相加得到最终的预测结果。
    2. 模型结构：AdaBoost通过线性组合多个弱分类器来构建一个强分类器，弱分类器之间没有依赖关系。GBDT通过级联多个回归树模型来构建一个强模型，每个模型都依赖于上一轮模型的预测结果。
    3. 样本权重：AdaBoost将错误分类的样本权重放大，使得这些样本在下一轮迭代中得到更多的关注，从而更容易被正确分类。GBDT不需要调整样本权重，因为每轮迭代训练的是上一轮模型的残差，而残差更容易被下一轮模型正确拟合。
    4. 损失函数：AdaBoost使用指数损失函数来调整样本权重，同时将多个弱分类器的输出进行加权平均。GBDT使用平方损失函数来拟合残差，同时将多个回归树模型的输出进行加权求和。
    5. 集成策略：AdaBoost通过加权表决的方式来集成多个弱分类器的输出，权重由分类器的准确率决定。GBDT通过级联多个回归树模型来构建一个强模型，每个模型的输出对最终结果的贡献是相加的。

### 模型

- **手推LR、Kmeans、SVM**
- **简述ridge和lasson的区别和联系**
- **树模型如何调参**
    
    树模型（包括决策树、随机森林、梯度提升树等）是一种基于树结构的机器学习模型，它们的超参数（即模型的参数之外的设置）对模型的性能和效率有重要影响。下面介绍一些调参的技巧：
    
    1. 树的深度和叶子节点个数：树的深度和叶子节点个数是控制树的复杂度的重要参数，它们直接影响模型的过拟合和欠拟合。可以通过交叉验证（Cross Validation）等方法来确定最优的深度和叶子节点个数。
    2. 特征子集大小：随机森林等集成学习模型中，每个决策树仅使用部分特征来进行划分，以降低模型的方差。可以通过交叉验证等方法来确定最优的特征子集大小。
    3. 内部节点最小样本数和叶子节点最小样本数：这两个参数控制了每个节点的最小样本数，从而可以控制树的生长。较小的值可以增加树的复杂度，较大的值可以减小过拟合的风险。可以通过交叉验证等方法来确定最优的最小样本数。
    4. 损失函数：梯度提升树等模型中，损失函数用于度量预测值与真实值之间的差距。不同的损失函数适用于不同的问题，比如均方误差适用于回归问题，交叉熵适用于分类问题。可以通过交叉验证等方法来确定最优的损失函数。
    5. 学习率和迭代次数：梯度提升树等模型中，学习率和迭代次数是两个重要的超参数。学习率控制每个基模型的权重，较小的学习率可以使模型更加稳定，但可能需要更多的迭代次数来达到最优解。迭代次数控制了模型的复杂度，可以通过交叉验证等方法来确定最优的学习率和迭代次数。
    6. 其他超参数：不同的树模型还有其他的超参数，比如决策树中的划分标准（比如基尼系数或信息熵），随机森林中的子采样比例，等等。可以通过交叉验证等方法来确定最优的超参数。
- **树模型如何剪枝？**
    
    树模型剪枝是一种减少模型复杂度，提高泛化能力的常见技术。常见的树模型剪枝算法包括预剪枝和后剪枝两种方法。
    
    预剪枝是在训练模型时，在树的构建过程中，根据一定的规则在节点的分裂前进行剪枝。预剪枝的主要思想是在决策树的生长过程中，当节点满足一定的条件时，不再进行分裂。具体来说，预剪枝的一些常见策略包括：
    
    1. 最大深度限制：限制树的最大深度。
    2. 最小样本数限制：限制节点的最小样本数，如果样本数量小于这个值，就不再进行分裂。
    3. 最小信息增益限制：限制节点分裂后的信息增益，如果信息增益小于这个值，就不再进行分裂。
    
    后剪枝是在训练完成后，对树进行修剪，去掉一些不必要的叶子节点，从而减少树的复杂度。具体来说，后剪枝的一些常见策略包括：
    
    1. 预测误差降低（PEP）：以测试集上的预测误差为评估标准，对叶子节点进行剪枝，如果剪枝后模型的预测误差没有变差，就进行剪枝。
    2. 等价子树剪枝：将同一层的兄弟节点合并成一个节点，然后计算合并前后模型的误差，如果误差没有变差，就进行剪枝。
    3. 最小化误差复杂度（MEC）：在PEP的基础上，增加一个正则化项，以平衡模型的预测误差和复杂度。
    
    需要注意的是，剪枝算法的效果往往受到数据集、模型结构、评估标准等多方面因素的影响，因此在具体应用中需要根据实际情况选择适当的剪枝算法和参数。
    
- **是否存一定存在参数，使得SVM的训练误差能到0**
    
    在SVM中，如果训练数据是线性可分的，即存在一个超平面能够完全将正负样本分开，那么可以通过调整SVM的参数，使得训练误差能够达到0。
    
    具体来说，如果训练数据是线性可分的，那么SVM的最优解就是所有支持向量的线性组合，而且SVM的分类决策函数是硬间隔。在这种情况下，可以通过设置一个较小的惩罚系数C，使得SVM更加关注正确分类，而不是最小化误差。
    
    当C趋近于无穷大时，SVM会将所有的数据点都分类正确，但这样可能会导致模型过于复杂，存在过拟合的风险。因此，在实际应用中，需要在训练误差为0的前提下，尽量选择一个较小的C值，从而使得模型具有更好的泛化能力。
    
- **逻辑回归如何处理多分类？**
    
    逻辑回归通常被用于处理二分类问题，但也可以用于多分类问题。逻辑回归解决多分类问题的主要方法是采用一对多（One-vs-Rest）或一对一（One-vs-One）策略。
    
    一对多策略也称为单分类策略。在这种策略下，对于K个类别中的每一个类别，都训练一个二分类模型。对于第i个类别，将所有属于该类别的样本标记为正例，将属于其他类别的样本标记为负例，然后训练一个二分类模型。这样，最终就会得到K个二分类模型，每个模型能够区分其中一个类别和其他所有类别的差异。在进行预测时，对于一个新的样本，将其输入到所有K个模型中，选择最高预测概率对应的类别作为分类结果。
    
    一对一策略是指对于K个类别中的每两个类别，训练一个二分类模型。对于第i个类别和第j个类别，将所有属于这两个类别的样本标记为正例，将属于其他类别的样本标记为负例，然后训练一个二分类模型。这样，最终就会得到K*(K-1)/2个二分类模型，每个模型能够区分其中两个类别的差异。在进行预测时，对于一个新的样本，将其输入到所有K*(K-1)/2个模型中，统计每个类别的得分，选择最高得分对应的类别作为分类结果。
    
    在实际应用中，一对多策略常常更常用，因为它具有较好的可扩展性和计算效率。而一对一策略由于需要训练更多的模型，可能会导致计算复杂度增加。
    
- **决策树有哪些划分指标？区别与联系？**
    
    决策树的划分指标是用来评价如何选择最优划分特征和划分点的指标，常用的划分指标包括信息熵、基尼指数和分类误差率。
    
    1. 信息熵（Entropy）：信息熵是度量样本集合纯度的指标，用来评价每个特征对样本分类的重要程度。信息熵越小，表示样本纯度越高，即样本中的大部分属于同一类别。计算公式为：$Ent(D)=-\sum_{k=1}^{|\mathcal{Y}|}p_klog_2p_k$，其中𝒴是所有类别的集合，*p*是样本中属于第k类的样本数占样本总数的比例。
        
        *k*
        
    2. 基尼指数（Gini Index）：基尼指数也是用来评价样本集合的纯度，与信息熵类似。基尼指数越小，表示样本纯度越高，即样本中的大部分属于同一类别。计算公式为：$Gini(D)=\sum_{k=1}^{|\mathcal{Y}|}\sum_{k'≠k}p_kp_{k'}=1-\sum_{k=1}^{|\mathcal{Y}|}p_k^2$，其中𝒴是所有类别的集合，*p*是样本中属于第k类的样本数占样本总数的比例。
        
        *k*
        
    3. 分类误差率（Classification Error）：分类误差率是用来评价样本集合的纯度的指标，与信息熵和基尼指数不同，分类误差率越小，表示样本纯度越高，即样本中的大部分属于同一类别。计算公式为：$Error(D)=1-\max\limits_{k∈\mathcal{Y}}p_k$，其中𝒴是所有类别的集合，*p*是样本中属于第k类的样本数占样本总数的比例。
        
        *k*
        
    
    这三种指标都是度量样本集合的不确定性，信息熵和基尼指数更常用，分类误差率使用较少。信息熵通常用于分类较多的情况，基尼指数则用于分类较少的情况。两者的计算复杂度相似，但基尼指数在计算上略微简单，因此在实践中更常用。
    
- **简述SVD和PCA的区别和联系？**
    
    SVD（Singular Value Decomposition）和PCA（Principal Component Analysis）是两种常用的降维方法，它们的作用是将高维数据映射到低维空间，以减少数据的维度并去除噪声和冗余信息。
    
    SVD和PCA的区别：
    
    1. 目标不同：SVD是一种矩阵分解技术，旨在将一个矩阵分解成三个矩阵的乘积，其中中间的矩阵是对角矩阵，对角线上的元素称为奇异值，它们表示矩阵的重要程度。而PCA是一种基于特征值分解的线性变换方法，它通过找到原始数据的主成分，将数据转化为一个新的坐标系，使得每个主成分之间不相关。
    2. 矩阵形式不同：SVD可以对任何形式的矩阵进行分解，包括矩阵的非对称矩阵和稠密矩阵等，而PCA只适用于对称正定矩阵。
    3. 计算方式不同：SVD的计算过程比PCA复杂，需要对原始数据进行奇异值分解，而PCA的计算过程比较简单，只需要对协方差矩阵进行特征值分解。
    
    SVD和PCA的联系：
    
    1. 两者都是用于降维的方法，可以将高维数据转化为低维数据，以减少数据的冗余信息和噪声。
    2. 两者都是基于线性代数的方法，可以用于多种数据类型，包括图片、声音、文本等。
    3. 在某些情况下，SVD和PCA可以互相替代。例如，对于对称正定矩阵，它们的结果是等价的，因为协方差矩阵就是一个对称正定矩阵。此外，PCA可以使用SVD来实现，因为PCA的计算可以看做是对协方差矩阵进行特征值分解。
- **如何使用梯度下降方法进行矩阵分解？**
- **LDA与PCA的区别与联系？**

### 特征工程

- **常用的特征筛选方法有哪些？**
    
    特征筛选是在机器学习和数据挖掘中用于减少特征数量，提高模型性能和减少过拟合的一种技术。以下是一些常用的特征筛选方法：
    
    1. 方差阈值：删除方差低于给定阈值的特征。这种方法适用于特征中有大量常数或接近常数的情况。
    2. 相关性阈值：删除与目标变量不相关的特征。可以使用皮尔逊相关系数或其他相关性测量指标来计算特征与目标变量之间的相关性。
    3. 卡方检验：卡方检验可用于评估分类变量之间的关系。可以使用卡方检验来评估每个特征与目标变量之间的关系，并删除关系不强的特征。
    4. 互信息：互信息可用于评估特征与目标变量之间的非线性关系。与卡方检验类似，可以使用互信息来评估每个特征与目标变量之间的关系，并删除关系不强的特征。
    5. L1 正则化：使用 L1 正则化可以对特征进行稀疏化，即使一部分特征的系数为零。使用 L1 正则化可以删除一些不重要的特征。
    6. 基于树的特征重要性：决策树和随机森林等基于树的算法可以计算特征的重要性。可以使用这些算法来评估每个特征对目标变量的影响，并删除重要性较低的特征。
    7. 基于模型的特征选择：可以使用任何基于模型的特征选择方法来评估每个特征对目标变量的影响，并删除不重要的特征。例如，可以使用 Lasso 回归、岭回归、逻辑回归等模型。
    
    这些方法并不是互斥的，可以组合使用，以提高特征筛选的效果。
    
- **文本如何构造特征？**
- **类别变量如何构造特征？**
- **连续值变量如何构造特征？**
- **哪些模型需要对特征进行归一化？**
- **什么是组合特征？如何处理高维组合特征**？

### 其他（分方向）

- **word2vec的原理，glove的原理，fasttext的原理？**
- **cbow和skipgram如何选择？**
- **了解elmo和bert吗？简述与word embedding的联系和区别**
- **图像和文本和语音数据各有哪些数据增强方法？**
- **rcnn、fatse rcnn、fatser rcnn、mask rcnn的原理？**
- **介绍resnet和GoogLeNet中的inception module的结构？**
- **介绍yolo和ssd ？**
    
    SSD（Single Shot Multibox Detector）是一种用于目标检测的神经网络模型，于2016年由Liu等人提出。它是一种基于深度学习的单阶段目标检测算法，具有较高的检测速度和精度。
    
    SSD主要有以下几个特点：
    
    1. 多尺度特征融合：SSD使用了一个多层卷积网络来提取图像特征，并且在不同层级上融合了不同分辨率的特征图。这样做的好处是可以在不同尺度下检测出不同大小的目标，同时减少了因尺度变化导致的检测误差。
    2. 先验框生成：SSD通过在不同尺度和不同长宽比下生成一组先验框（默认框）来确定检测目标的位置。这样做可以提高检测速度，并且可以减少误检。
    3. 多层特征融合：SSD使用了多个卷积层来提取不同尺度的特征，这些特征在不同层级上进行融合。这种多层特征融合的方法可以提高检测精度，并且可以在不同层级上检测出不同尺度的目标。
    4. 位置回归和类别预测：SSD在每个先验框上同时预测其所属类别和位置信息。这种方式可以避免多次扫描图像，从而提高了检测速度。同时，位置回归可以提高检测精度。
    5. Hard Negative Mining：在训练过程中，SSD会选择一些难以分类的负样本进行训练，以避免出现过多的假阳性。
    
    总的来说，SSD是一种基于深度学习的目标检测算法，它通过多尺度特征融合、先验框生成、多层特征融合、位置回归和类别预测等技术，实现了较高的检测精度和速度。
    
- **介绍FM，FFM，deepFM，deepWide.**
- **机器翻译如何解决oov？**

## Transfomer

- **为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？意义是什么？**
    
    在获取输入词向量后，将词向量矩阵乘以embedding size的开方是为了对词向量的范围进行缩放。这种缩放的目的是为了控制词向量的方差，使其有适当的尺度，以便更好地适应不同的神经网络模型。
    
    主要的原因如下：
    
    1. 控制方差：词向量的方差可以影响模型的训练稳定性。如果词向量的方差过大，可能导致模型训练过程中的梯度爆炸问题。通过对词向量矩阵乘以embedding size的开方，可以使词向量的方差适中，并降低梯度爆炸的风险。
    2. 数值稳定性：在神经网络模型中，各层的输入和输出的数值范围通常需要保持相对稳定。通过缩放词向量的范围，可以确保输入的数值范围在合理的范围内，避免模型的数值不稳定性问题。
    3. 梯度传播：词向量的范围可以影响梯度的传播。如果词向量的范围过大或过小，可能会导致梯度在网络中传播时出现问题，例如梯度消失或梯度爆炸。通过缩放词向量的范围，可以促进梯度在不同层之间更好地传播。
    
    需要注意的是，针对每个具体的任务或模型，对词向量进行缩放的具体方式可能会不同。上述将词向量矩阵乘以embedding size的开方只是常见的一种方式，实际情况可能因任务需求、模型架构等因素而有所差异。
    
- **简单介绍一下Transformer的位置编码？有什么意义和优缺点？**
    
    Transformer的位置编码是为了将序列中每个词的位置信息引入到模型中。在Transformer模型中，由于自注意力机制的使用，模型无法通过传统的位置索引来理解输入序列中的顺序信息。因此，通过位置编码来表示序列中每个位置的信息，以便模型能够感知序列中词的相对位置。
    
    Transformer使用的位置编码是通过在输入的词向量中添加一个位置向量来实现的。位置向量的维度与词向量的维度相同，其设计灵感来自于正弦和余弦函数。具体来说，位置编码使用了一组固定的公式计算得出，使得不同位置上的位置向量在空间中的表示有所不同。
    
    位置编码的意义和优缺点如下：
    
    意义：
    
    1. 引入位置信息：位置编码能够传达序列中每个位置的顺序信息，使模型能够区分不同位置上的词，并获取它们之间的相对距离。
    2. 模型泛化能力：位置编码的引入使得Transformer模型能够处理可变长度的序列输入，因为位置编码并不依赖于特定的输入序列长度。
    
    优点：
    
    1. 无需训练：位置编码是固定的，并不需要随训练一起调整参数。这样可以减少模型的训练时间和计算量。
    2. 捕获位置关系：位置编码能够帮助模型准确地表示词的相对位置，捕捉到距离和顺序的信息。
    
    缺点：
    
    1. 无法处理超出训练范围的序列长度：位置编码是通过固定的公式计算得到的，无法处理超出训练过程中已见过的最长序列长度的输入。
    2. 位置信息建模有限：位置编码只能提供词之间相对位置的信息，而无法提供绝对位置的信息。在某些特定任务中，绝对位置可能对模型的性能有重要影响。
    
    总体而言，位置编码在Transformer中为模型引入了位置信息，使其能够处理顺序相关的任务。该方法简单且高效，但仍存在一些限制，如对序列长度的限制和无法提供绝对位置信息。
    
- **简单讲一下Transformer中的残差结构以及意义。**
    
    残差结构的意义如下：
    
    1. 缓解梯度消失：深层神经网络中，通过层层堆叠的变换可能导致梯度逐渐变小，从而导致梯度消失问题。残差结构通过将原始输入直接传递给下一层，使得梯度能够更轻松地通过网络传播，从而缓解了梯度消失问题。
    2. 促进信息传递：残差结构保留了原始输入的信息，将其与经过子层变换后的输出相加。这样做的好处是能够保留更多的细节信息，有助于模型学习更多的细微特征，并促进信息在网络中的传递。
    3. 简化优化：残差结构使得每个子层的输出能够更接近于一个恒等映射，因为如果子层的变换没有提供更多的信息，输入将直接传递给下一层。这简化了网络的优化问题，降低了训练的难度。
- **Transformer的并行化提现在哪个地方？Decoder端可以做并行化吗？**
    
    在Transformer中，Encoder和Decoder都使用了自注意力机制。在Encoder中，每个位置的注意力权重是根据整个输入序列计算得出的，因此可以进行并行计算。这使得Encoder能够并行地处理整个输入序列，从而加快训练和推理速度。
    
    而在Decoder端，因为每个位置的输出依赖于之前已生成的位置，所以无法像Encoder那样直接进行并行计算。然而，仍然存在一些可以进行并行化的方法来提高Decoder的效率。
    
    一种常见的并行化方法是使用“Masked Self-Attention”，即在Decoder的自注意力机制中通过掩码将未来的位置（即当前位置之后的位置）屏蔽掉。这样可以确保在生成当前位置的输出时，只考虑前面已经生成的位置。
    
    此外，还有一种称为“Parallel Decoding”的方法，通过将未来的位置的注意力权重设置为0，可以在一定程度上实现Decoder端的并行化。这种方法可以利用多个GPU或多个计算单元同时生成输出。
    
    需要注意的是，Decoder的并行化仍然有一定限制，并不像Encoder那样完全并行计算。由于生成输出时的依赖关系，Decoder端的并行化可能会导致一些计算行之间的依赖关系，从而降低并行效率。
    
- **AadmW详解**
    
    Adamw 即 Adam + weight decate ,效果与 Adam + L2[正则化](https://so.csdn.net/so/search?q=%E6%AD%A3%E5%88%99%E5%8C%96&spm=1001.2101.3001.7020)相同,但是计算效率更高,因为L2正则化需要在loss中加入正则项,之后再算梯度,最后在反向传播,而Adamw直接将正则项的梯度加入反向传播的公式中,省去了手动在loss中加正则项这一步
    
- **线程与进程的区别，在python中用的是什么包？**
    
    在线程与进程的区别方面，线程是程序执行的最小单位，而进程是操作系统分配资源的最小单位。一个进程可以包含多个线程，它们共享进程的资源，如内存空间、文件句柄等。每个线程有自己的执行路径，但它们可以访问共享的数据。
    
    线程之间的切换比进程快，因为线程共享相同的内存空间，而进程之间的切换需要进行上下文切换和内存切换，开销更大。另外，由于线程共享内存，因此线程之间的通信比进程更方便。但是，线程之间的共享数据也带来了线程安全的问题，需要额外的同步机制来避免竞争条件。
    
    在Python中，用于处理线程和进程的主要包是`threading`和`multiprocessing`。`threading`包提供了线程相关的功能，包括创建线程、线程同步、线程间通信等。`multiprocessing`包用于处理进程相关的操作，它提供了创建进程、进程间通信、进程池等功能。这两个包都是Python标准库的一部分，可以在Python中直接使用。
    
- **有哪些tokenizer的方式**
    - word-level
    - char-level
    - subword-level:**尽量不分解常用词，而是将不常用词分解为常用的子词**，“annoyingly”可能被认为是一个罕见的单词，并且可以分解为“annoying”和“ly”。“annoying”并“ly”作为独立的子词会更频繁地出现。
        
        subword的分词往往包含了两个阶段,一个是encode阶段,形成subword的vocabulary dict,一个是decode阶段,将原始的文本通过subword的vocabulary dict 转化为 token的index然后进入embedding层.
        
    
    subword-level 最为常用，Llama的tokenizer是BPE。**BPE每一步都将最常见的一对*相邻数据单位*替换为该数据中没有出现过的一个*新单位* ，反复迭代直到满足停止条件。**
    
    合并字符可以让你**用最少的token来表示语料库**，这也是 BPE 算法的主要目标，即**数据的压缩**。在每个单词的末尾添加“</w>”标记以标识单词边界能够让算法知道**每个单词的结束位置**（因为我们统计相邻字符对时不能把**分别位于两个单词**中的字符对算进去），这有助于算法查看每个字符并找到频率最高的字符配对。“</w>”也能被算作字符对的一部分。
    
- **transformer为什么要用三个不一样的QKV？**
    
    K和Q的点乘是为了得到一个attention score 矩阵，用来对V进行提纯。K和Q使用了不同的W_k, W_Q来计算，可以理解为是在不同空间上的投影。正因为有了这种不同空间的投影，增加了表达能力，这样计算得到的attention score矩阵的泛化能力更高。这里解释下我理解的泛化能力，因为K和Q使用了不同的W_k, W_Q来计算，得到的也是两个完全不同的矩阵，所以表达能力更强。
    
    但是如果不用Q，直接拿K和K点乘的话，你会发现attention score 矩阵是一个对称矩阵。因为是同样一个矩阵，都投影到了同样一个空间，所以泛化能力很差。这样的矩阵导致对V进行提纯的时候，效果也不会好。
    
- **Transformer attention的注意力矩阵的计算为什么用乘法而不是加法？**
    
    为了计算更快。
    
    加法形式是先加、后tanh、再和V矩阵相乘，相当于一个完整的隐层。
    
    在计算复杂度上，乘法和加法理论上的复杂度相似，但是在实践中，乘法可以利用高度优化的矩阵乘法代码（有成熟的加速实现）使得点乘速度更快，空间利用率更高。（论文P4有解释）
    
    在dkd_kdk较小的时候，加法和乘法形式效果相近。但是随着 dkd_kdk增大，加法开始显著优于乘法。作者认为，dkd_kdk增大导致乘法性能不佳的原因，是**极大的点乘值将整个softmax推向梯度平缓区**，使得收敛困难。于是选择scale，除dkdk。
    
- **Transformer中是怎么做multi head attention 的，这样做multi head attention，会增加它的时间复杂度嘛？**
    - Q1：怎么做的？
    - A: 把输入映射为多组QKV矩阵，每组分别计算注意力，再将每组的结果concat起来。
    - Q2：会增加时间复杂度吗？
    - A: 不会。Self—Attn的时间复杂度为: $O(n2⋅d),$ 这里，$n$是序列长度，$d$是Embedding的维度。
    - Sef-Attn包括三个步骤：相似度计算，Softmax和加权平均。它们的时间复杂度是：
    - 相似度计算可以看作大小为$(n,d)$和$(d,n)$的两个矩阵相乘：$(n,d)×(d,n)=O(n2⋅d)$，得到一个$(n,n)$的矩阵。
    - softmax的时间复杂度为$O(n2⋅d)$。
    - 加权平均的每一项可以看作大小为$(n,n)$和$(n,d)的$两个矩阵相乘: $(n,n)×(n,d)=O(n2⋅d)$
    - 所以，自注意力总的时间复杂度为$O(n2⋅d)$。
    
    而多头并不是循环计算每个头，而是通过transpose and reshapes（即经过矩阵矩阵的变换），再用矩阵乘法来完成的。
    
    原本（$n,d$）的输入，经过矩阵变换，拆解成维度为$(n,h,a)$的的矩阵，$h$为头的数量，$a$为每个头输入的维度，再调整n和h的顺序得到（$h,n,a）$的矩阵作为每个头的输入。
    
    这样，两个矩阵相乘$(h,n,a)×(h,a,n)$可以看做是两个小矩阵相乘$(n,a)×(a,n)$做$h$次，所以时间复杂度为$O(n2⋅h)=O(n2⋅d$)。
    
- **zeroshot和Fewshot具体做法的区别？**
    
    Zeroshot和Fewshot是两种在自然语言处理任务中处理未见过的类别或样本的方法，它们有以下具体做法上的区别：
    
    1. Zeroshot（零样本学习）：
        - Zeroshot方法旨在处理完全未见过的类别或样本。在训练阶段，模型接收到类别和样本的描述信息，但没有直接观察到这些类别或样本的实例。然后，模型通过学习从已见过的类别中抽取的一般化知识来进行预测。
        - 一种常见的Zeroshot方法是使用外部知识库或语义表示来表示类别和样本，例如使用WordNet、知识图谱或预训练的词向量。模型可以通过将问题或样本与这些表示进行匹配来进行预测。
    2. Fewshot（少样本学习）：
        - Fewshot方法旨在处理只有少量样本的类别。在训练阶段，模型只接收到少量样本和对应的类别标签。这些样本可以是从已见过的类别中随机选择的，或者是通过一些特定的采样策略选择的。模型需要从这些少量样本中学习一般化的表示和模式，以便在测试阶段处理未见过的类别。
        - Fewshot方法通常使用元学习（meta-learning）的思想，通过在少量样本的任务上进行多个迭代的训练，使模型能够快速适应新的类别。元学习算法可以帮助模型学习到一般化的优化策略，以在少样本任务上获得较好的性能。

## 算法基础

- **Hash表的时间复杂度为什么是O(1)？**
    
    !https://s2.51cto.com/images/blog/202110/15170112_616943580cc8c76136.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=/format,webp/resize,m_fixed,w_1184
    

 hash表是基于**数组+链表**的实现的。数组在内存中是一块连续的空间，只要知道查找数据的下标就可快速定位到数据的内存地址，即数组查找数据的时间复杂度为O(1)。hash表的存储结构是<key，value>的形式，数据读取时，只需提供key就可快速查找到value。hash表依据数组利用下标快读查找数据的特性来实现这样的查找方式的。也就是如上图中所示，**hash表的物理存储其实是数组**。

- 排序的时间复杂度
    
    
    | 排序算法 | 平均时间复杂度 | 最坏时间复杂度 | 最好时间复杂度 | 空间复杂度 | 稳定性 |
    | --- | --- | --- | --- | --- | --- |
    | 冒泡排序 | O(n²) | O(n²) | O(n) | O(1) | 稳定 |
    | 直接选择排序 | O(n²) | O(n²) | O(n) | O(1) | 不稳定 |
    | 直接插入排序 | O(n²) | O(n²) | O(n) | O(1) | 稳定 |
    | 快速排序 | O(nlogn) | O(n²) | O(nlogn) | O(nlogn) | 不稳定 |
    | 堆排序 | O(nlogn) | O(nlogn) | O(nlogn) | O(1) | 不稳定 |
    | 希尔排序 | O(nlogn) | O(ns) | O(n) | O(1) | 不稳定 |
    | 归并排序 | O(nlogn) | O(nlogn) | O(nlogn) | O(n) | 稳定 |
    | 计数排序 | O(n+k) | O(n+k) | O(n+k) | O(n+k) | 稳定 |
    | 基数排序 | O(N*M) | O(N*M) | O(N*M) | O(M) | 稳定 |
- 数据结构的时间复杂度
