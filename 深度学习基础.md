# 深度学习基础

- **如何划分训练集？如何选取验证集？**
    
    一般来说，我们将数据集划分为训练集、验证集和测试集三部分。其中，训练集用于训练模型，验证集用于评估模型的性能和调整模型参数，测试集用于最终评估模型的性能。
    
    - 留出法（Hold-out）：将数据集划分为训练集和验证集两部分。一般来说，将数据集的70% ~ 80%作为训练集，剩余的20% ~ 30%作为验证集。
    - 交叉验证法（Cross-validation）：将数据集划分为k个子集，其中k-1个子集用于训练模型，剩余的一个子集用于验证模型。将k个子集分别作为验证集，重复k次，最终得到k个模型性能的评估结果。
    - 自助法（Bootstrapping）：从数据集中随机抽取一个样本，并将其放入训练集中，然后将样本放回数据集中，继续进行下一次抽样。这样，训练集中可能包含重复的样本，验证集中则包含未被选中的样本。
        1. 验证集应该与测试集和实际数据具有相似的分布。
        2. 验证集的大小应该足够大，以便对模型进行准确的评估。
        3. 验证集应该与训练集互斥，即在训练集中没有出现过的样本应该被包含在验证集中。
        4. 如果使用交叉验证法，验证集应该尽可能包含数据集中的所有样本，以便对模型进行全面的评估。
- **什么是偏差和方差？**
    1. 误差（ Error） = 偏差（ Bias） + 方差（ Variance） + 噪声（ Noise），一般地，我们把机器学习模型的预测输出与样本的真实label之间的差异称为误差，其反应的是整个模型的准确度。
    2. 噪声（ Noise）：描述了在当前任务上任何机器学习算法所能达到的期望泛化误差的下界，即刻画了当前任务本质的难度。
    3. 偏差（ Bias）：衡量了模型拟合训练数据的能力，偏差反应的是所有采样得到的大小相同的训练集训练出的所有模型的输出平均值和真实label之间的偏差，即模型本身的精确度。偏差通常是由于我们对机器学习算法做了错误的假设所导致的，比如真实数据分布映射的是某个 二次函数，但我们假设模型是一次函数。
    4. 偏差（ Bias）越小，拟合能力却强（可能产生过拟合）；反之，拟合能力越弱（可能产生欠拟合） 。偏差越大，越偏离真实数据。
    5. 方差描述的是预测值的变化范围，离散程度，也就是离期望值的距离。 方差越大，数据的分布越分散，模型的稳定程度越差。
    6. 方差也反应了模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。 由方差带来的 误差通常体现在测试误差相对于训练误差的增量上。
    7. 方差通常是由于模型的复杂度相对于训练样本数过高导致的。 方差越小，模型的泛化能力越高； 反之，模型的泛化能力越低。如果模型在训练集上拟合效果比较优秀，但是在测试集上拟合效果比较差， 则表示方差较大， 说明模型的稳定程度较差，出现这种现象可能是由于模型对训练集过拟合造成的。
- **什么是过拟合？深度学习解决过拟合的方法有哪些？**
    
    过拟合（Overfitting）是指机器学习算法在训练数据上表现良好，但在测试数据上表现差的现象。通常发生在模型过于复杂、训练数据量较少、训练次数过多等情况下。
    
    深度学习解决过拟合的方法有以下几种：
    
    1. 增加训练数据：通过增加训练数据，可以有效地降低模型过拟合的风险。这是最有效的方法之一，但有时候增加训练数据并不是一件容易的事情。
    2. 正则化（Regularization）：通过在模型的损失函数中添加正则项来限制模型的复杂度，例如L1正则化、L2正则化等。正则化可以强制模型更加平滑，减少模型对噪声的敏感性。
    3. Dropout：Dropout是一种在训练过程中随机删除一部分神经元的方法。这样可以使得模型不能过于依赖某些特征，从而降低过拟合的风险。
    4. 提前停止（Early stopping）：在训练过程中，可以在验证集的性能不再提高时停止训练，从而避免过拟合。这个方法有时可以在训练速度和模型效果之间做出一个平衡。
    5. 数据增强（Data augmentation）：通过对原始训练数据进行旋转、平移、缩放等变换，生成更多的训练数据。这样可以增加数据的多样性，帮助模型更好地学习。
    6. 模型集成（Model ensemble）：通过将多个模型的预测结果进行融合，可以得到更准确的结果。这个方法需要训练多个模型，因此比较耗时和复杂。
- **简述了解的优化器，发展综述？**
    
    优化器是深度学习中用于优化模型参数的算法。随着深度学习的发展，越来越多的优化器被提出并应用于不同类型的神经网络，以提高训练效率和准确性。下面是一些常见的优化器和它们的发展历程：
    
    1. 梯度下降法（Gradient Descent, GD）：是优化器中最基础的一种，它通过计算损失函数的梯度来更新模型参数，使得损失函数下降。随着深度学习的发展，衍生出了批量梯度下降（Batch Gradient Descent, BGD）、随机梯度下降（Stochastic Gradient Descent, SGD）和小批量梯度下降（Mini-batch Gradient Descent, MBGD）等变种。
    2. 动量法（Momentum）：在梯度下降的基础上引入了动量，以加速学习的过程。在更新参数时，除了考虑当前梯度，还考虑之前的梯度方向，并对之前的梯度方向进行加权平均。这种方法可以减小梯度方向的变化，从而使得训练更加平滑。
    3. 自适应学习率算法：自适应学习率算法可以根据梯度的大小来自动调整学习率，从而更好地适应不同的数据集和网络结构。其中，Adagrad、Adadelta、RMSprop 和 Adam 等是比较流行的算法。
    4. Adamw 即 Adam + weight decate ,效果与 Adam + L2正则化相同,但是计算效率更高,因为L2正则化需要在loss中加入正则项,之后再算梯度,最后在反向传播，
        
        而Adamw直接将正则项的梯度加入反向传播的公式中,省去了手动在loss中加正则项这一步。
        
- **以一层隐层的神经网络，relu激活，MSE作为损失函数推导反向传播**
    
    设训练样本集为$*D = (x(1), y(1)), (x(2), y(2)), ..., (x(n), y(n))*$，其中*x*(*i*)是输入，$*y(i)*$是对应的目标输出，*wij*表示输入层到隐层的第*i*个神经元与输出层的第*j*个神经元之间的权重，$*bj*$表示输出层的第*j*个神经元的偏置项，$*aj*$表示输出层的第$*j*$个神经元的输出，$*hi*$表示隐层的第$*i*$个神经元的输出，$*f( ⋅ )*$表示ReLU激活函数，$*L(y(i), a(i))*$表示第*i*个样本的损失函数（均方误差）。
    
    前向传播过程：
    
    首先，输入层的输出就是输入样本*x*(*i*)：
    
    $*hi = xi*$
    
    接着，隐层的输出通过线性变换计算得到：
    
    $u_j=\sum_{i=1}^{d}w_{ij}h_i+b_j$
    
    其中，$*d*$表示输入层的神经元个数，$*j = 1, 2, ..., m*$表示隐层的神经元个数。
    
    然后，隐层的输出通过ReLU激活函数进行非线性变换得到：
    
    $*f(uj) = max(0, uj)*$
    
    最后，输出层的输出通过线性变换计算得到：
    
    $a_j=\sum_{i=1}^{m}w_{ij}f(u_i)$
    
    反向传播过程：
    
    首先，计算输出层的误差项：
    
    $\delta_j=\frac{\partial L(y^{(i)},a_j)}{\partial a_j}=\frac{\partial}{\partial a_j}\frac{1}{2}(y^{(i)}-a_j)^2=-(y^{(i)}-a_j)$
    
    然后，根据链式法则计算隐层的误差项：
    
    $\delta_i=\frac{\partial L}{\partial u_i}=\frac{\partial L}{\partial a_j}\frac{\partial a_j}{\partial u_i}=\delta_jw_{ij}f'(u_i)$
    
    其中，$*f′( ⋅ )*$表示ReLU激活函数的导数：
    
    $f'(u_i)=\begin{cases}1,&u_i>0\\0,&u_i\leq0\end{cases}$
    
    接着，计算输出层到隐层之间的权重梯度：
    
    $\frac{\partial L}{\partial w_{ij}}=\frac{\partial L}{\partial a_j}\frac{\partial a_j}{\partial u_i}h_i=\delta_jf(u_i)$
    
    最后，计算输出层的偏置项梯度和隐层的偏置项梯度：
    
    $\frac{\partial L}{\partial b_j}=\delta_j$
    
- **NN的权重参数能否初始化为0？**
    - 通常情况下，不建议将神经网络的权重参数初始化为0。这是因为，如果所有的权重参数都初始化为0，那么每个神经元在前向传播时都会输出相同的结果，导致所有层的输出都相同，使得网络失去了表达能力。此外，由于反向传播过程中的梯度计算是基于权重参数进行的，如果所有的权重参数都相同，则每个权重参数在反向传播过程中都会受到相同的更新，导致权重参数无法学习。
    - 因此，在神经网络的训练中，权重参数的初始化通常需要采用一些随机的方法，例如高斯分布、均匀分布等。这样可以确保不同的神经元在前向传播时输出不同的结果，增强网络的表达能力，也可以帮助权重参数学习不同的特征，提高网络的准确性。
    - 需要注意的是，虽然不建议将所有权重参数初始化为0，但有些情况下可以将某些层的权重参数初始化为0，例如全连接层中的偏置参数可以初始化为0。这是因为偏置参数只对应一个神经元，而不像权重参数对应多个神经元，因此对于偏置参数初始化为0不会导致所有神经元的输出相同。但是，在实际应用中，通常使用其他初始化方法来初始化偏置参数，例如均匀分布、高斯分布等。
- **什么是梯度消失和梯度爆炸？**
    - 梯度消失指的是在反向传播过程中，梯度逐渐变小，最终变得非常小，无法对神经网络的权重进行有效的更新。这个问题通常出现在深度神经网络中，尤其是在使用sigmoid或tanh等S型激活函数时更为明显。因为这些函数的导数在接近0的地方变得非常小，当网络很深时，梯度不断乘以小于1的数会导致梯度趋近于0。
    - 梯度爆炸则相反，指的是在反向传播过程中，梯度逐渐变大，最终变得非常大，使得神经网络的权重值变化非常快，导致训练不稳定。梯度爆炸通常发生在循环神经网络（RNN）等结构中，因为这些结构具有很强的时序依赖性，当梯度在时间上传递时，会导致梯度指数级增长。
    - 为了解决这些问题，一些方法被提出，如ReLU激活函数的使用和批量归一化（Batch Normalization）等技术，它们可以帮助缓解梯度消失和梯度爆炸的问题。此外，还有其他技术，如梯度剪切（Gradient Clipping）等，可以限制梯度的大小，防止梯度爆炸。
- **常用的激活函数，导数？**
    1. Sigmoid 函数：
        
        Sigmoid 函数可以将实数值映射到 (0, 1) 的区间内。其公式为：
        
        $\sigma(x)=\frac{1}{1+e^{-x}}$
        
        其导数为：
        
        $\frac{\partial}{\partial x}\sigma(x)=\sigma(x)\cdot(1-\sigma(x))$
        
    2. ReLU 函数：
        
        ReLU 函数可以将负值直接置为 0，可以有效地避免梯度消失问题。其公式为：
        
        ReLU(*x*) = max (0, *x*)
        
        其导数为：
        
        $\frac{\partial}{\partial x}\text{ReLU}(x)=\begin{cases} 1 & (x>0) \ 0 & (x\leq0) \end{cases}$
        
        虽然ReLU激活函数在输入为0的点上不可导，但这在实际中并不会对神经网络的训练带来太大影响。在实现中，可以将输入为0的点的导数定义为0，从而避免对反向传播算法的影响。
        
    3. Leaky ReLU 函数：
        
        Leaky ReLU 函数是对 ReLU 函数的改进，可以避免出现 ReLU 函数中的神经元“死亡”的问题。其公式为：
        
        $\text{LeakyReLU}(x)=\begin{cases} x & (x>0) \ \alpha x & (x\leq0) \end{cases}$
        
        其中，*α* 为一个小于 1 的常数，通常取 0.01。
        
        其导数为：
        
        $\frac{\partial}{\partial x}\text{LeakyReLU}(x)=\begin{cases} 1 & (x>0) \ \alpha & (x\leq0) \end{cases}$
        
    4. Tanh 函数：
        
        Tanh 函数可以将实数值映射到 ( − 1, 1) 的区间内。其公式为：
        
        $\text{tanh}(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$
        
        其导数为：
        
        $\frac{\partial}{\partial x}\text{tanh}(x)=1-\text{tanh}^2(x)$
        
- **relu的有优点？又有什么局限性？他们的系列改进方法是啥？**
    
    ReLU（Rectified Linear Unit）是一种常用的激活函数，它具有以下优点：
    
    1. 计算速度快：ReLU函数的计算非常简单，只需要进行一次max运算即可，相比于Sigmoid和tanh等激活函数，ReLU的计算速度更快。
    2. 解决梯度消失问题：ReLU的导数在正区间恒为1，因此在正区间可以避免梯度消失问题，使得神经网络可以更快地学习。
    3. 收敛速度快：由于ReLU函数的性质，使得梯度下降算法可以更快地收敛。
    
    虽然ReLU函数具有以上优点，但也存在一些局限性：
    
    1. 死亡ReLU现象：当输入的值为负数时，ReLU函数的导数为0，称为“死亡ReLU”，此时该神经元将不再对模型产生贡献，可能会导致网络性能下降。
    2. 输出不是均值为0：ReLU的输出范围是[0,∞)，不是以0为中心的，可能会导致神经网络输出的分布不均衡。
    
    为了解决ReLU存在的问题，有许多改进方法被提出，例如：
    
    1. Leaky ReLU：在负区间，使用一个小的斜率，使得ReLU函数在负区间也有一定的导数。
    2. PReLU（Parametric ReLU）：与Leaky ReLU类似，但将斜率变成可学习的参数，通过反向传播算法优化。
    3. ELU（Exponential Linear Unit）：在负区间使用一个指数函数，使得在负区间有较小的导数。
    4. Maxout：使用多个ReLU函数取最大值，相当于学习多个ReLU函数，有更强的拟合能力。
    
    这些改进方法可以一定程度上解决ReLU存在的问题，提升神经网络的性能。
    
- **sigmoid和tanh为什么会导致梯度消失？**
    
    Sigmoid和tanh是两种常用的激活函数，它们都具有“S”形状的曲线，且取值范围都在[-1, 1]之间。在神经网络中，这两个函数通常用于激活隐藏层的输出。然而，这些函数也存在梯度消失的问题。
    
    梯度消失是指在反向传播过程中，由于梯度逐层相乘，导致低层神经元的梯度变得非常小，甚至接近于零，使得它们的参数更新几乎不起作用，从而导致训练变得非常缓慢，甚至停滞不前。这个问题在深度神经网络中尤其严重，因为它们有许多层。
    
    Sigmoid和tanh函数都具有以下特点：
    
    - 在输入非常大或非常小的时候，函数的梯度会变得非常小，接近于零，导致梯度消失的问题。
    - 函数的导数在输入接近零的时候取值最大，但是当输入偏离零点越远，导数的值就越小，最终趋近于零。
    
    这些特点使得在深度神经网络中使用sigmoid和tanh激活函数时，梯度消失的问题非常严重。为了解决这个问题，人们提出了一些新的激活函数，比如ReLU、LeakyReLU和ELU等，它们在输入较大或较小的时候都具有较大的梯度，从而缓解了梯度消失的问题。
    
- **dropout为何能防止过拟合？**
    - 过拟合是机器学习中常见的问题，当模型在训练数据上表现良好但在测试数据上表现较差时，就会出现过拟合。Dropout是一种防止过拟合的技术，其基本原理是在训练过程中随机丢弃一部分神经元，从而减少神经元之间的依赖关系，防止模型对训练数据的过拟合。
    - 具体来说，Dropout的实现是在每个训练批次中随机选择一些神经元不参与训练，即将其输出值设为0。这些被随机丢弃的神经元会在下一次训练中重新参与训练，从而提高模型的鲁棒性和泛化能力。
    - Dropout的作用类似于集成学习中的bagging方法，通过随机选择不同的神经元组合，从而减少模型对任意一组神经元的依赖性，进而降低模型的方差，防止模型对训练数据的过拟合。同时，Dropout也可以使模型学习到更加鲁棒的特征表示，提高模型的泛化能力。
    - 需要注意的是，Dropout只应该在训练过程中使用，在测试过程中应该关闭Dropout，保留所有的神经元，这样才能得到模型的真实预测结果。
- **dropout和 BN 在前向传播和反向传播阶段的区别？**
    
    Dropout和Batch Normalization（BN）是两种常用的正则化技术，在神经网络中被广泛使用。它们在前向传播和反向传播阶段的实现方式有所不同。
    
    在前向传播阶段，Dropout和BN的实现方式如下：
    
    - Dropout：在每个训练批次中，以一定的概率（通常为0.5）随机丢弃神经网络中的一些节点（通常是隐藏层的节点），即将它们的输出值置为零。这样可以随机地让一些神经元失活，从而减少过拟合的风险。在测试阶段，Dropout不再起作用，而是使用整个网络来进行推理。
    - BN：对于每个特征维度，计算其均值和方差，并将其标准化为零均值和单位方差的分布。然后对其进行缩放和平移，即乘以一个尺度参数和加上一个偏移参数，从而让网络学习到更好的特征表示。在训练和测试阶段都使用。
    
    在反向传播阶段，Dropout和BN的实现方式如下：
    
    - Dropout：在反向传播过程中，只传递未被丢弃的神经元的梯度，其他神经元的梯度被置为零。这相当于在每个训练批次中，以一定的概率随机地让一些神经元失活，从而减少过拟合的风险。在测试阶段，Dropout不再起作用，因此无需进行反向传播。
    - BN：在反向传播过程中，计算每个特征维度的梯度，并将其传递到上一层。BN可以看作是一种平移和缩放操作，因此需要同时更新尺度参数和偏移参数的梯度。这样可以让网络学习到更好的特征表示，从而加快收敛速度并降低过拟合的风险。
    
    综上所述，Dropout和BN在前向传播和反向传播阶段的实现方式略有不同，但它们都可以有效地防止过拟合，并提高神经网络的性能。